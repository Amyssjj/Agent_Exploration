{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1PmahGx-COSu-42u2CbyUQDVyILSaTEfS","timestamp":1733425443405},{"file_id":"1gR77QyS93MmrXnf8w9hh6AX6H1bCjRM1","timestamp":1733248593684},{"file_id":"16k-Bda4Fg3KkC0rKw47TJapjugaD6FUj","timestamp":1732063745022}],"authorship_tag":"ABX9TyOZKwXbFl5SaWVfua3xDM+U"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!git config --global user.name 'Amyssjj'\n","!git config --global user.email 'amysj1983@gmail.com'\n","!git config --global user.password '013310Lyf@'"],"metadata":{"id":"wo38c3MoT1E4","executionInfo":{"status":"ok","timestamp":1733444462537,"user_tz":480,"elapsed":421,"user":{"displayName":"amy shi","userId":"00695257177288089305"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["token = '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'\n","username = 'Amyssjj'\n","repo = 'Agent_Influcencer'"],"metadata":{"id":"cPkcs2plUyM1","executionInfo":{"status":"ok","timestamp":1733444464037,"user_tz":480,"elapsed":133,"user":{"displayName":"amy shi","userId":"00695257177288089305"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["!git clone https://{token}@github.com/{username}/{repo}"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gzEuh2vXVnD3","executionInfo":{"status":"ok","timestamp":1733444621924,"user_tz":480,"elapsed":763,"user":{"displayName":"amy shi","userId":"00695257177288089305"}},"outputId":"c574efa1-f946-4d7e-916a-6046501c4595"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'Agent_Influcencer'...\n","remote: Enumerating objects: 29, done.\u001b[K\n","remote: Counting objects: 100% (29/29), done.\u001b[K\n","remote: Compressing objects: 100% (25/25), done.\u001b[K\n","remote: Total 29 (delta 5), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n","Receiving objects: 100% (29/29), 498.09 KiB | 6.15 MiB/s, done.\n","Resolving deltas: 100% (5/5), done.\n"]}]},{"cell_type":"code","source":["%cd {repo}"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ahdD-XP3W-KQ","executionInfo":{"status":"ok","timestamp":1733444766608,"user_tz":480,"elapsed":3,"user":{"displayName":"amy shi","userId":"00695257177288089305"}},"outputId":"f7d30da7-fa5c-4118-e488-95150a9cb392"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/Agent_Influcencer\n"]}]},{"cell_type":"code","source":["%ls"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5Mc1o5UWXB6o","executionInfo":{"status":"ok","timestamp":1733444775683,"user_tz":480,"elapsed":128,"user":{"displayName":"amy shi","userId":"00695257177288089305"}},"outputId":"63f20529-db7b-4d2b-f136-ac6d4d01f685"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["README.md  \u001b[0m\u001b[01;34m_static\u001b[0m/\n"]}]},{"cell_type":"code","source":["!mv /content/drive/MyDrive/Colab Notebooks/Agent - Learn insights from Youtube video"],"metadata":{"id":"buXYvVJoXHS7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **Pre-requisition**"],"metadata":{"id":"jmTAQ0CZq43c"}},{"cell_type":"markdown","source":["# **Youtube Insights Learning buddy**\n","\n","#### **Motivation: In such a fast-paced world where information on the internet is exploding, staying connecte, forming my own perspective, and staying sharp is incredibly challenging. Yet, with full passion for learning and growth, I’m always striving to improve.**\n","\n","\n","<hr>        \n","\n","\n","\n","\n"],"metadata":{"id":"3t9BCdH2MLbU"}},{"cell_type":"code","source":["# **Cheat-sheet for Google Colab**"],"metadata":{"id":"qmyeyGW8MKBh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install youtube_transcript_api elevenlabs langfun --pre"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"f5MsEykb6NKG","executionInfo":{"status":"ok","timestamp":1733425514204,"user_tz":480,"elapsed":17569,"user":{"displayName":"amy shi","userId":"00695257177288089305"}},"outputId":"22125922-6725-4761-fca6-e905498fa9de"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting youtube_transcript_api\n","  Downloading youtube_transcript_api-0.6.3-py3-none-any.whl.metadata (17 kB)\n","Collecting elevenlabs\n","  Downloading elevenlabs-1.13.3-py3-none-any.whl.metadata (8.0 kB)\n","Collecting langfun\n","  Downloading langfun-0.1.2.dev202412050804-py3-none-any.whl.metadata (8.3 kB)\n","Requirement already satisfied: defusedxml<0.8.0,>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from youtube_transcript_api) (0.7.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from youtube_transcript_api) (2.32.3)\n","Requirement already satisfied: httpx>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from elevenlabs) (0.28.0)\n","Requirement already satisfied: pydantic>=1.9.2 in /usr/local/lib/python3.10/dist-packages (from elevenlabs) (2.10.2)\n","Requirement already satisfied: pydantic-core<3.0.0,>=2.18.2 in /usr/local/lib/python3.10/dist-packages (from elevenlabs) (2.27.1)\n","Requirement already satisfied: typing_extensions>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from elevenlabs) (4.12.2)\n","Collecting websockets>=11.0 (from elevenlabs)\n","  Downloading websockets-14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n","Collecting pyglove>=0.4.5.dev202409110000 (from langfun)\n","  Downloading pyglove-0.4.5.dev202412050809-py3-none-any.whl.metadata (6.7 kB)\n","Requirement already satisfied: jinja2>=3.1.2 in /usr/local/lib/python3.10/dist-packages (from langfun) (3.1.4)\n","Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.21.2->elevenlabs) (3.7.1)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.21.2->elevenlabs) (2024.8.30)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.21.2->elevenlabs) (1.0.7)\n","Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx>=0.21.2->elevenlabs) (3.10)\n","Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.21.2->elevenlabs) (0.14.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=3.1.2->langfun) (3.0.2)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9.2->elevenlabs) (0.7.0)\n","Requirement already satisfied: docstring-parser>=0.12 in /usr/local/lib/python3.10/dist-packages (from pyglove>=0.4.5.dev202409110000->langfun) (0.16)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->youtube_transcript_api) (3.4.0)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->youtube_transcript_api) (2.2.3)\n","Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.21.2->elevenlabs) (1.3.1)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.21.2->elevenlabs) (1.2.2)\n","Downloading youtube_transcript_api-0.6.3-py3-none-any.whl (622 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m622.3/622.3 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading elevenlabs-1.13.3-py3-none-any.whl (211 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.1/211.1 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading langfun-0.1.2.dev202412050804-py3-none-any.whl (388 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m388.7/388.7 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pyglove-0.4.5.dev202412050809-py3-none-any.whl (655 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m655.1/655.1 kB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading websockets-14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (168 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.2/168.2 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: websockets, pyglove, youtube_transcript_api, langfun, elevenlabs\n","Successfully installed elevenlabs-1.13.3 langfun-0.1.2.dev202412050804 pyglove-0.4.5.dev202412050809 websockets-14.1 youtube_transcript_api-0.6.3\n"]}]},{"cell_type":"code","source":["# Let's add path of virtual environment site-packages to system path\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import sys\n","sys.path.append(\"/content/drive/MyDrive/jing_env_colab/colab_env/lib/python3.10/site-packages\")\n","sys.path.append('/content/drive/MyDrive/ColabNotebooks')  # Replace with the folder path\n","\n","\n","import langfun as lf\n","import pyglove as pg\n","from typing import Literal\n","import pandas as pd\n","import numpy as np\n","import os\n","import google.generativeai as genai\n","from google.colab import userdata\n","from langfun.core.structured import function_generation"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qv-aNbbInET_","executionInfo":{"status":"ok","timestamp":1733441493918,"user_tz":480,"elapsed":7644,"user":{"displayName":"amy shi","userId":"00695257177288089305"}},"outputId":"8b49cc7e-87ea-46f4-9448-94ab6d1f3149"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:root:`pg.Object.schema` is deprecated and will be removed in future. Please use `__schema__` instead.\n"]}]},{"cell_type":"markdown","source":["# **Build your crew to help you achieve your goal,  choose them wisely!**"],"metadata":{"id":"S7W9Az4CZZoR"}},{"cell_type":"code","source":["## Get the key and LLMs\n","gemini_key = userdata.get('gemini_key')\n","claude_key = userdata.get('claude_key')\n","openai_key = userdata.get('openai_key')\n","youtube_key = userdata.get('youtube_key')\n","\n","## This is your crew, help to achieve your goal/tasks!! Choose them wisely, since each of them have different expertise.\n","lm_claude = lf.llms.Claude35Sonnet20241022(api_key=claude_key, temperature=0.6) ## good at coding\n","lm_openai = lf.llms.Gpt4o(api_key = openai_key, temperature=0.6) ## good at summarization"],"metadata":{"id":"vL6PFY7Wo-ay"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Task 1 - Help you Write the code to retrieval all the information from a Youtube Video**\n"],"metadata":{"id":"FgRcXDXVZlC0"}},{"cell_type":"code","source":["code = lf.query(prompt = \"Help me to write a python code that achieve retreival the youtube video basic info, caption, and comments\",  # Your Request: What to generate code for\n","                lm = lm_claude,   # Your LLM Crew:  eg, Use Claude to write code\n","                schema=lf.PythonCode,  # Output: should be Python code\n","    )"],"metadata":{"id":"P1cxXQjLN_8a","executionInfo":{"status":"ok","timestamp":1733438049745,"user_tz":480,"elapsed":8163,"user":{"displayName":"amy shi","userId":"00695257177288089305"}}},"execution_count":51,"outputs":[]},{"cell_type":"code","source":["code"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"VSKXyA8oO4DD","executionInfo":{"status":"ok","timestamp":1733438052258,"user_tz":480,"elapsed":159,"user":{"displayName":"amy shi","userId":"00695257177288089305"}},"outputId":"c8076747-7232-4ed4-9d42-410826ae97fb"},"execution_count":52,"outputs":[{"output_type":"execute_result","data":{"text/plain":["PythonCode(source='\\nfrom googleapiclient.discovery import build\\nfrom youtube_transcript_api import YouTubeTranscriptApi\\n\\n# Set up YouTube API client\\nAPI_KEY = \\'YOUR_API_KEY\\'  # Replace with your actual API key\\nyoutube = build(\\'youtube\\', \\'v3\\', developerKey=API_KEY)\\n\\ndef get_video_info(video_id):\\n    # Get video details\\n    video_response = youtube.videos().list(\\n        part=\\'snippet,statistics\\',\\n        id=video_id\\n    ).execute()\\n    \\n    video_info = video_response[\\'items\\'][0]\\n    \\n    # Get video captions\\n    try:\\n        captions = YouTubeTranscriptApi.get_transcript(video_id)\\n    except:\\n        captions = \"No captions available\"\\n        \\n    # Get video comments\\n    comments = []\\n    try:\\n        comments_response = youtube.commentThreads().list(\\n            part=\\'snippet\\',\\n            videoId=video_id,\\n            maxResults=100\\n        ).execute()\\n        \\n        for item in comments_response[\\'items\\']:\\n            comment = item[\\'snippet\\'][\\'topLevelComment\\'][\\'snippet\\']\\n            comments.append({\\n                \\'author\\': comment[\\'authorDisplayName\\'],\\n                \\'text\\': comment[\\'textDisplay\\'],\\n                \\'likes\\': comment[\\'likeCount\\'],\\n                \\'published_at\\': comment[\\'publishedAt\\']\\n            })\\n    except:\\n        comments = \"Comments are disabled for this video\"\\n    \\n    return {\\n        \\'title\\': video_info[\\'snippet\\'][\\'title\\'],\\n        \\'description\\': video_info[\\'snippet\\'][\\'description\\'], \\n        \\'view_count\\': video_info[\\'statistics\\'][\\'viewCount\\'],\\n        \\'like_count\\': video_info[\\'statistics\\'][\\'likeCount\\'],\\n        \\'comment_count\\': video_info[\\'statistics\\'].get(\\'commentCount\\', 0),\\n        \\'captions\\': captions,\\n        \\'comments\\': comments\\n    }\\n\\n# Example usage\\nvideo_id = \\'VIDEO_ID\\'  # Replace with actual YouTube video ID\\nvideo_data = get_video_info(video_id)\\n\\n# Print results\\nprint(\\'Title:\\', video_data[\\'title\\'])\\nprint(\\'View count:\\', video_data[\\'view_count\\'])\\nprint(\\'Captions:\\', video_data[\\'captions\\'])\\nprint(\\'Comments:\\', video_data[\\'comments\\'])\\n')"],"text/html":["<html>\n","<head>\n","<style>\n","/* Tooltip styles. */\n","span.tooltip {\n","  visibility: hidden;\n","  white-space: pre-wrap;\n","  font-weight: normal;\n","  background-color: #484848;\n","  color: #fff;\n","  padding: 10px;\n","  border-radius: 6px;\n","  position: absolute;\n","  z-index: 1;\n","}\n","span.tooltip:hover {\n","  visibility: visible;\n","}\n","/* Summary styles. */\n","details.pyglove summary {\n","  font-weight: bold;\n","  margin: -0.5em -0.5em 0;\n","  padding: 0.5em;\n","}\n",".summary-name {\n","  display: inline;\n","  padding: 3px 5px 3px 5px;\n","  margin: 0 5px;\n","  border-radius: 3px;\n","}\n",".summary-title {\n","  display: inline;\n","}\n",".summary-name + div.summary-title {\n","  display: inline;\n","  color: #aaa;\n","}\n",".summary-title:hover + span.tooltip {\n","  visibility: visible;\n","}\n",".summary-name:hover > span.tooltip {\n","  visibility: visible;\n","  background-color: darkblue;\n","}\n","/* Simple value styles. */\n",".simple-value {\n","  color: blue;\n","  display: inline-block;\n","  white-space: pre-wrap;\n","  padding: 0.2em;\n","  margin-top: 0.15em;\n","}\n",".simple-value.str {\n","  color: darkred;\n","  font-style: italic;\n","}\n",".simple-value.int, .simple-value.float {\n","  color: darkblue;\n","}\n","/* Value details styles. */\n","details.pyglove {\n","  border: 1px solid #aaa;\n","  border-radius: 4px;\n","  padding: 0.5em 0.5em 0;\n","  margin: 0.25em 0;\n","}\n","details.pyglove[open] {\n","  padding: 0.5em 0.5em 0.5em;\n","}\n",".highlight {\n","  background-color: Mark;\n","}\n",".lowlight {\n","  opacity: 0.2;\n","}\n","/* Complex value styles. */\n","span.empty-container::before {\n","    content: '(empty)';\n","    font-style: italic;\n","    margin-left: 0.5em;\n","    color: #aaa;\n","}\n","</style>\n","</head>\n","<body>\n","<details open class=\"pyglove python-code\"><summary><div class=\"summary-title\">PythonCode(...)</div><span class=\"tooltip\">PythonCode(\n","  source=&quot;\\nfrom googleapiclient.discovery import build\\nfrom youtube_transcript_api import YouTubeTranscriptApi\\n\\n# Set up YouTube API client\\nAPI_KEY = &#x27;YOUR_API_KEY&#x27;  # Replace with your actual API key\\nyoutube = build(&#x27;youtube&#x27;, &#x27;v3&#x27;, developerKey=API_KEY)\\n\\ndef get_v...&quot;\n",")</span></summary><div class=\"complex-value python-code\"><details open class=\"pyglove str\"><summary><div class=\"summary-name\">source<span class=\"tooltip\">source</span></div><div class=\"summary-title\">str</div><span class=\"tooltip\">&quot;\\nfrom googleapiclient.discovery import build\\nfrom youtube_transcript_api import YouTubeTranscriptApi\\n\\n# Set up YouTube API client\\nAPI_KEY = &#x27;YOUR_API_KEY&#x27;  # Replace with your actual API key\\nyoutube = build(&#x27;youtube&#x27;, &#x27;v3&#x27;, developerKey=API_KEY)\\n\\ndef get_v...&quot;</span></summary><span class=\"simple-value str\">\n","from googleapiclient.discovery import build\n","from youtube_transcript_api import YouTubeTranscriptApi\n","\n","# Set up YouTube API client\n","API_KEY = &#x27;YOUR_API_KEY&#x27;  # Replace with your actual API key\n","youtube = build(&#x27;youtube&#x27;, &#x27;v3&#x27;, developerKey=API_KEY)\n","\n","def get_video_info(video_id):\n","    # Get video details\n","    video_response = youtube.videos().list(\n","        part=&#x27;snippet,statistics&#x27;,\n","        id=video_id\n","    ).execute()\n","    \n","    video_info = video_response[&#x27;items&#x27;][0]\n","    \n","    # Get video captions\n","    try:\n","        captions = YouTubeTranscriptApi.get_transcript(video_id)\n","    except:\n","        captions = &quot;No captions available&quot;\n","        \n","    # Get video comments\n","    comments = []\n","    try:\n","        comments_response = youtube.commentThreads().list(\n","            part=&#x27;snippet&#x27;,\n","            videoId=video_id,\n","            maxResults=100\n","        ).execute()\n","        \n","        for item in comments_response[&#x27;items&#x27;]:\n","            comment = item[&#x27;snippet&#x27;][&#x27;topLevelComment&#x27;][&#x27;snippet&#x27;]\n","            comments.append({\n","                &#x27;author&#x27;: comment[&#x27;authorDisplayName&#x27;],\n","                &#x27;text&#x27;: comment[&#x27;textDisplay&#x27;],\n","                &#x27;likes&#x27;: comment[&#x27;likeCount&#x27;],\n","                &#x27;published_at&#x27;: comment[&#x27;publishedAt&#x27;]\n","            })\n","    except:\n","        comments = &quot;Comments are disabled for this video&quot;\n","    \n","    return {\n","        &#x27;title&#x27;: video_info[&#x27;snippet&#x27;][&#x27;title&#x27;],\n","        &#x27;description&#x27;: video_info[&#x27;snippet&#x27;][&#x27;description&#x27;], \n","        &#x27;view_count&#x27;: video_info[&#x27;statistics&#x27;][&#x27;viewCount&#x27;],\n","        &#x27;like_count&#x27;: video_info[&#x27;statistics&#x27;][&#x27;likeCount&#x27;],\n","        &#x27;comment_count&#x27;: video_info[&#x27;statistics&#x27;].get(&#x27;commentCount&#x27;, 0),\n","        &#x27;captions&#x27;: captions,\n","        &#x27;comments&#x27;: comments\n","    }\n","\n","# Example usage\n","video_id = &#x27;VIDEO_ID&#x27;  # Replace with actual YouTube video ID\n","video_data = get_video_info(video_id)\n","\n","# Print results\n","print(&#x27;Title:&#x27;, video_data[&#x27;title&#x27;])\n","print(&#x27;View count:&#x27;, video_data[&#x27;view_count&#x27;])\n","print(&#x27;Captions:&#x27;, video_data[&#x27;captions&#x27;])\n","print(&#x27;Comments:&#x27;, video_data[&#x27;comments&#x27;])\n","</span></details></div></details>\n","</body>\n","</html>"]},"metadata":{},"execution_count":52}]},{"cell_type":"code","source":["## Simply copy paste the code above,  and replace the API_KEY and Youtube_id you need to fetch!\n","\n","from googleapiclient.discovery import build\n","from youtube_transcript_api import YouTubeTranscriptApi\n","\n","# Replace with your API key\n","API_KEY = youtube_key\n","\n","def get_video_info(video_id):\n","    # Create YouTube API client\n","    youtube = build('youtube', 'v3', developerKey=API_KEY)\n","\n","    # Get video details\n","    video_response = youtube.videos().list(\n","        part='snippet,statistics',\n","        id=video_id\n","    ).execute()\n","\n","    if not video_response['items']:\n","        return None\n","\n","    video_info = video_response['items'][0]\n","\n","    # Get video captions\n","    try:\n","        captions = YouTubeTranscriptApi.get_transcript(video_id)\n","    except:\n","        captions = None\n","\n","    # Get video comments\n","    comments = []\n","    try:\n","        comments_response = youtube.commentThreads().list(\n","            part='snippet',\n","            videoId=video_id,\n","            maxResults=100\n","        ).execute()\n","\n","        for item in comments_response['items']:\n","            comment = item['snippet']['topLevelComment']['snippet']\n","            comments.append({\n","                'author': comment['authorDisplayName'],\n","                'text': comment['textDisplay'],\n","                'likes': comment['likeCount'],\n","                'published_at': comment['publishedAt']\n","            })\n","    except:\n","        pass\n","\n","    return {\n","        'title': video_info['snippet']['title'],\n","        'description': video_info['snippet']['description'],\n","        'view_count': video_info['statistics']['viewCount'],\n","        'like_count': video_info['statistics']['likeCount'],\n","        'comment_count': video_info['statistics']['commentCount'],\n","        'captions': captions,\n","        'comments': comments\n","    }\n","\n"],"metadata":{"id":"CsV1EmfeO_vt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Example usage:\n","video_id = 'VMj-3S1tku0'  # Replace with actual video ID\n","video_data = get_video_info(video_id)\n","video_data"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"C4b7r0DAQECf","executionInfo":{"status":"ok","timestamp":1733427645449,"user_tz":480,"elapsed":1348,"user":{"displayName":"amy shi","userId":"00695257177288089305"}},"outputId":"27f45f37-ffe3-4280-b819-93cfeeafa5f1"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'title': 'The spelled-out intro to neural networks and backpropagation: building micrograd',\n"," 'description': 'This is the most step-by-step spelled-out explanation of backpropagation and training of neural networks. It only assumes basic knowledge of Python and a vague recollection of calculus from high school.\\n\\nLinks:\\n- micrograd on github: https://github.com/karpathy/micrograd\\n- jupyter notebooks I built in this video: https://github.com/karpathy/nn-zero-to-hero/tree/master/lectures/micrograd\\n- my website: https://karpathy.ai\\n- my twitter: https://twitter.com/karpathy\\n- \"discussion forum\": nvm, use youtube comments below for now :)\\n- (new) Neural Networks: Zero to Hero series Discord channel: https://discord.gg/3zy8kqD9Cp , for people who\\'d like to chat more and go beyond youtube comments\\n\\nExercises:\\nyou should now be able to complete the following google collab, good luck!:\\nhttps://colab.research.google.com/drive/1FPTx1RXtBfc4MaTkf7viZZD4U2F9gtKN?usp=sharing\\n\\nChapters:\\n00:00:00 intro\\n00:00:25 micrograd overview\\n00:08:08 derivative of a simple function with one input\\n00:14:12 derivative of a function with multiple inputs\\n00:19:09 starting the core Value object of micrograd and its visualization\\n00:32:10 manual backpropagation example #1: simple expression\\n00:51:10 preview of a single optimization step\\n00:52:52 manual backpropagation example #2: a neuron\\n01:09:02 implementing the backward function for each operation\\n01:17:32 implementing the backward function for a whole expression graph\\n01:22:28 fixing a backprop bug when one node is used multiple times\\n01:27:05 breaking up a tanh, exercising with more operations\\n01:39:31 doing the same thing but in PyTorch: comparison\\n01:43:55 building out a neural net library (multi-layer perceptron) in micrograd\\n01:51:04 creating a tiny dataset, writing the loss function\\n01:57:56 collecting all of the parameters of the neural net\\n02:01:12 doing gradient descent optimization manually, training the network\\n02:14:03 summary of what we learned, how to go towards modern neural nets\\n02:16:46 walkthrough of the full code of micrograd on github\\n02:21:10 real stuff: diving into PyTorch, finding their backward pass for tanh\\n02:24:39 conclusion\\n02:25:20 outtakes :)',\n"," 'view_count': '2059994',\n"," 'like_count': '46117',\n"," 'comment_count': '1933',\n"," 'captions': [{'text': 'hello my name is andre',\n","   'start': 0.08,\n","   'duration': 2.88},\n","  {'text': \"and i've been training deep neural\",\n","   'start': 1.839,\n","   'duration': 3.041},\n","  {'text': 'networks for a bit more than a decade',\n","   'start': 2.96,\n","   'duration': 3.839},\n","  {'text': \"and in this lecture i'd like to show you\",\n","   'start': 4.88,\n","   'duration': 3.759},\n","  {'text': 'what neural network training looks like',\n","   'start': 6.799,\n","   'duration': 4.081},\n","  {'text': 'under the hood so in particular we are',\n","   'start': 8.639,\n","   'duration': 3.601},\n","  {'text': 'going to start with a blank jupiter',\n","   'start': 10.88,\n","   'duration': 3.44},\n","  {'text': 'notebook and by the end of this lecture',\n","   'start': 12.24,\n","   'duration': 4.4},\n","  {'text': 'we will define and train in neural net',\n","   'start': 14.32,\n","   'duration': 3.84},\n","  {'text': \"and you'll get to see everything that\",\n","   'start': 16.64,\n","   'duration': 3.6},\n","  {'text': 'goes on under the hood and exactly',\n","   'start': 18.16,\n","   'duration': 3.52},\n","  {'text': 'sort of how that works on an intuitive',\n","   'start': 20.24,\n","   'duration': 2.4},\n","  {'text': 'level', 'start': 21.68, 'duration': 2.48},\n","  {'text': 'now specifically what i would like to do',\n","   'start': 22.64,\n","   'duration': 3.92},\n","  {'text': 'is i would like to take you through',\n","   'start': 24.16,\n","   'duration': 5.199},\n","  {'text': 'building of micrograd now micrograd is',\n","   'start': 26.56,\n","   'duration': 4.32},\n","  {'text': 'this library that i released on github',\n","   'start': 29.359,\n","   'duration': 3.521},\n","  {'text': 'about two years ago but at the time i',\n","   'start': 30.88,\n","   'duration': 4.0},\n","  {'text': \"only uploaded the source code and you'd\",\n","   'start': 32.88,\n","   'duration': 4.4},\n","  {'text': 'have to go in by yourself and really',\n","   'start': 34.88,\n","   'duration': 4.4},\n","  {'text': 'figure out how it works', 'start': 37.28, 'duration': 3.36},\n","  {'text': 'so in this lecture i will take you',\n","   'start': 39.28,\n","   'duration': 3.119},\n","  {'text': 'through it step by step and kind of',\n","   'start': 40.64,\n","   'duration': 3.759},\n","  {'text': 'comment on all the pieces of it so what',\n","   'start': 42.399,\n","   'duration': 5.041},\n","  {'text': 'is micrograd and why is it interesting',\n","   'start': 44.399,\n","   'duration': 4.561},\n","  {'text': 'good', 'start': 47.44, 'duration': 2.4},\n","  {'text': 'um', 'start': 48.96, 'duration': 2.56},\n","  {'text': 'micrograd is basically an autograd',\n","   'start': 49.84,\n","   'duration': 4.0},\n","  {'text': 'engine autograd is short for automatic',\n","   'start': 51.52,\n","   'duration': 4.16},\n","  {'text': 'gradient and really what it does is it',\n","   'start': 53.84,\n","   'duration': 3.84},\n","  {'text': 'implements backpropagation now',\n","   'start': 55.68,\n","   'duration': 3.679},\n","  {'text': 'backpropagation is this algorithm that',\n","   'start': 57.68,\n","   'duration': 3.6},\n","  {'text': 'allows you to efficiently evaluate the',\n","   'start': 59.359,\n","   'duration': 4.161},\n","  {'text': 'gradient of', 'start': 61.28, 'duration': 3.919},\n","  {'text': 'some kind of a loss function with',\n","   'start': 63.52,\n","   'duration': 3.68},\n","  {'text': 'respect to the weights of a neural',\n","   'start': 65.199,\n","   'duration': 3.92},\n","  {'text': 'network and what that allows us to do',\n","   'start': 67.2,\n","   'duration': 3.84},\n","  {'text': 'then is we can iteratively tune the',\n","   'start': 69.119,\n","   'duration': 3.441},\n","  {'text': 'weights of that neural network to',\n","   'start': 71.04,\n","   'duration': 3.28},\n","  {'text': 'minimize the loss function and therefore',\n","   'start': 72.56,\n","   'duration': 3.919},\n","  {'text': 'improve the accuracy of the network so',\n","   'start': 74.32,\n","   'duration': 4.0},\n","  {'text': 'back propagation would be at the',\n","   'start': 76.479,\n","   'duration': 4.0},\n","  {'text': 'mathematical core of any modern deep',\n","   'start': 78.32,\n","   'duration': 4.24},\n","  {'text': 'neural network library like say pytorch',\n","   'start': 80.479,\n","   'duration': 3.521},\n","  {'text': 'or jaxx', 'start': 82.56, 'duration': 3.04},\n","  {'text': 'so the functionality of microgrant is i',\n","   'start': 84.0,\n","   'duration': 3.6},\n","  {'text': 'think best illustrated by an example so',\n","   'start': 85.6,\n","   'duration': 4.24},\n","  {'text': 'if we just scroll down here', 'start': 87.6, 'duration': 3.6},\n","  {'text': \"you'll see that micrograph basically\",\n","   'start': 89.84,\n","   'duration': 3.12},\n","  {'text': 'allows you to build out mathematical',\n","   'start': 91.2,\n","   'duration': 3.04},\n","  {'text': 'expressions', 'start': 92.96, 'duration': 3.6},\n","  {'text': 'and um here what we are doing is we have',\n","   'start': 94.24,\n","   'duration': 3.6},\n","  {'text': \"an expression that we're building out\",\n","   'start': 96.56,\n","   'duration': 4.32},\n","  {'text': 'where you have two inputs a and b',\n","   'start': 97.84,\n","   'duration': 5.44},\n","  {'text': \"and you'll see that a and b are negative\",\n","   'start': 100.88,\n","   'duration': 5.279},\n","  {'text': 'four and two but we are wrapping those',\n","   'start': 103.28,\n","   'duration': 5.36},\n","  {'text': 'values into this value object that we',\n","   'start': 106.159,\n","   'duration': 3.761},\n","  {'text': 'are going to build out as part of',\n","   'start': 108.64,\n","   'duration': 2.4},\n","  {'text': 'micrograd', 'start': 109.92, 'duration': 3.6},\n","  {'text': 'so this value object will wrap the',\n","   'start': 111.04,\n","   'duration': 3.92},\n","  {'text': 'numbers themselves', 'start': 113.52, 'duration': 2.72},\n","  {'text': 'and then we are going to build out a',\n","   'start': 114.96,\n","   'duration': 3.519},\n","  {'text': 'mathematical expression here where a and',\n","   'start': 116.24,\n","   'duration': 5.68},\n","  {'text': 'b are transformed into c d and',\n","   'start': 118.479,\n","   'duration': 5.441},\n","  {'text': 'eventually e f and g', 'start': 121.92, 'duration': 3.68},\n","  {'text': \"and i'm showing some of the functions\",\n","   'start': 123.92,\n","   'duration': 3.12},\n","  {'text': 'some of the functionality of micrograph',\n","   'start': 125.6,\n","   'duration': 3.359},\n","  {'text': 'and the operations that it supports so',\n","   'start': 127.04,\n","   'duration': 3.999},\n","  {'text': 'you can add two value objects you can',\n","   'start': 128.959,\n","   'duration': 4.401},\n","  {'text': 'multiply them you can raise them to a',\n","   'start': 131.039,\n","   'duration': 4.721},\n","  {'text': 'constant power you can offset by one',\n","   'start': 133.36,\n","   'duration': 5.28},\n","  {'text': 'negate squash at zero', 'start': 135.76, 'duration': 5.76},\n","  {'text': 'square divide by constant divide by it',\n","   'start': 138.64,\n","   'duration': 4.0},\n","  {'text': 'etc', 'start': 141.52, 'duration': 2.799},\n","  {'text': \"and so we're building out an expression\",\n","   'start': 142.64,\n","   'duration': 4.72},\n","  {'text': 'graph with with these two inputs a and b',\n","   'start': 144.319,\n","   'duration': 6.0},\n","  {'text': \"and we're creating an output value of g\",\n","   'start': 147.36,\n","   'duration': 5.28},\n","  {'text': 'and micrograd will in the background',\n","   'start': 150.319,\n","   'duration': 3.841},\n","  {'text': 'build out this entire mathematical',\n","   'start': 152.64,\n","   'duration': 3.76},\n","  {'text': 'expression so it will for example know',\n","   'start': 154.16,\n","   'duration': 4.56},\n","  {'text': 'that c is also a value', 'start': 156.4, 'duration': 4.96},\n","  {'text': 'c was a result of an addition operation',\n","   'start': 158.72,\n","   'duration': 4.08},\n","  {'text': 'and the', 'start': 161.36, 'duration': 5.28},\n","  {'text': 'child nodes of c are a and b because the',\n","   'start': 162.8,\n","   'duration': 6.079},\n","  {'text': 'and will maintain pointers to a and b',\n","   'start': 166.64,\n","   'duration': 4.16},\n","  {'text': \"value objects so we'll basically know\",\n","   'start': 168.879,\n","   'duration': 4.481},\n","  {'text': 'exactly how all of this is laid out',\n","   'start': 170.8,\n","   'duration': 4.64},\n","  {'text': 'and then not only can we do what we call',\n","   'start': 173.36,\n","   'duration': 3.68},\n","  {'text': 'the forward pass where we actually look',\n","   'start': 175.44,\n","   'duration': 3.2},\n","  {'text': \"at the value of g of course that's\",\n","   'start': 177.04,\n","   'duration': 3.52},\n","  {'text': 'pretty straightforward we will access',\n","   'start': 178.64,\n","   'duration': 4.959},\n","  {'text': 'that using the dot data attribute and so',\n","   'start': 180.56,\n","   'duration': 5.52},\n","  {'text': 'the output of the forward pass the value',\n","   'start': 183.599,\n","   'duration': 5.761},\n","  {'text': 'of g is 24.7 it turns out but the big',\n","   'start': 186.08,\n","   'duration': 5.84},\n","  {'text': 'deal is that we can also take this g',\n","   'start': 189.36,\n","   'duration': 4.48},\n","  {'text': 'value object and we can call that',\n","   'start': 191.92,\n","   'duration': 2.959},\n","  {'text': 'backward', 'start': 193.84, 'duration': 2.88},\n","  {'text': 'and this will basically uh initialize',\n","   'start': 194.879,\n","   'duration': 4.961},\n","  {'text': 'back propagation at the node g',\n","   'start': 196.72,\n","   'duration': 4.64},\n","  {'text': 'and what backpropagation is going to do',\n","   'start': 199.84,\n","   'duration': 3.36},\n","  {'text': \"is it's going to start at g and it's\",\n","   'start': 201.36,\n","   'duration': 3.84},\n","  {'text': 'going to go backwards through that',\n","   'start': 203.2,\n","   'duration': 3.52},\n","  {'text': \"expression graph and it's going to\",\n","   'start': 205.2,\n","   'duration': 3.599},\n","  {'text': 'recursively apply the chain rule from',\n","   'start': 206.72,\n","   'duration': 3.36},\n","  {'text': 'calculus', 'start': 208.799, 'duration': 3.44},\n","  {'text': 'and what that allows us to do then is',\n","   'start': 210.08,\n","   'duration': 4.159},\n","  {'text': \"we're going to evaluate basically the\",\n","   'start': 212.239,\n","   'duration': 4.321},\n","  {'text': 'derivative of g with respect to all the',\n","   'start': 214.239,\n","   'duration': 3.92},\n","  {'text': 'internal nodes', 'start': 216.56, 'duration': 4.399},\n","  {'text': 'like e d and c but also with respect to',\n","   'start': 218.159,\n","   'duration': 5.041},\n","  {'text': 'the inputs a and b', 'start': 220.959, 'duration': 4.401},\n","  {'text': 'and then we can actually query this',\n","   'start': 223.2,\n","   'duration': 4.72},\n","  {'text': 'derivative of g with respect to a for',\n","   'start': 225.36,\n","   'duration': 4.799},\n","  {'text': \"example that's a dot grad in this case\",\n","   'start': 227.92,\n","   'duration': 4.56},\n","  {'text': 'it happens to be 138 and the derivative',\n","   'start': 230.159,\n","   'duration': 4.08},\n","  {'text': 'of g with respect to b', 'start': 232.48, 'duration': 4.959},\n","  {'text': 'which also happens to be here 645',\n","   'start': 234.239,\n","   'duration': 5.2},\n","  {'text': \"and this derivative we'll see soon is\",\n","   'start': 237.439,\n","   'duration': 3.921},\n","  {'text': \"very important information because it's\",\n","   'start': 239.439,\n","   'duration': 5.44},\n","  {'text': 'telling us how a and b are affecting g',\n","   'start': 241.36,\n","   'duration': 5.519},\n","  {'text': 'through this mathematical expression so',\n","   'start': 244.879,\n","   'duration': 3.201},\n","  {'text': 'in particular', 'start': 246.879, 'duration': 4.961},\n","  {'text': 'a dot grad is 138 so if we slightly',\n","   'start': 248.08,\n","   'duration': 6.719},\n","  {'text': 'nudge a and make it slightly larger',\n","   'start': 251.84,\n","   'duration': 6.16},\n","  {'text': '138 is telling us that g will grow and',\n","   'start': 254.799,\n","   'duration': 4.641},\n","  {'text': 'the slope of that growth is going to be',\n","   'start': 258.0,\n","   'duration': 2.72},\n","  {'text': '138', 'start': 259.44, 'duration': 3.52},\n","  {'text': 'and the slope of growth of b is going to',\n","   'start': 260.72,\n","   'duration': 4.88},\n","  {'text': \"be 645. so that's going to tell us about\",\n","   'start': 262.96,\n","   'duration': 4.88},\n","  {'text': 'how g will respond if a and b get',\n","   'start': 265.6,\n","   'duration': 4.159},\n","  {'text': 'tweaked a tiny amount in a positive',\n","   'start': 267.84,\n","   'duration': 3.28},\n","  {'text': 'direction', 'start': 269.759, 'duration': 3.601},\n","  {'text': 'okay', 'start': 271.12, 'duration': 2.24},\n","  {'text': 'now you might be confused about what',\n","   'start': 273.44,\n","   'duration': 3.36},\n","  {'text': 'this expression is that we built out',\n","   'start': 274.88,\n","   'duration': 3.68},\n","  {'text': 'here and this expression by the way is',\n","   'start': 276.8,\n","   'duration': 3.92},\n","  {'text': 'completely meaningless i just made it up',\n","   'start': 278.56,\n","   'duration': 3.68},\n","  {'text': \"i'm just flexing about the kinds of\",\n","   'start': 280.72,\n","   'duration': 3.039},\n","  {'text': 'operations that are supported by',\n","   'start': 282.24,\n","   'duration': 2.64},\n","  {'text': 'micrograd', 'start': 283.759, 'duration': 2.561},\n","  {'text': 'what we actually really care about are',\n","   'start': 284.88,\n","   'duration': 3.2},\n","  {'text': 'neural networks but it turns out that',\n","   'start': 286.32,\n","   'duration': 3.52},\n","  {'text': 'neural networks are just mathematical',\n","   'start': 288.08,\n","   'duration': 3.839},\n","  {'text': 'expressions just like this one but',\n","   'start': 289.84,\n","   'duration': 5.04},\n","  {'text': 'actually slightly bit less crazy even',\n","   'start': 291.919,\n","   'duration': 4.401},\n","  {'text': 'neural networks are just a mathematical',\n","   'start': 294.88,\n","   'duration': 4.16},\n","  {'text': 'expression they take the input data as',\n","   'start': 296.32,\n","   'duration': 4.4},\n","  {'text': 'an input and they take the weights of a',\n","   'start': 299.04,\n","   'duration': 3.439},\n","  {'text': \"neural network as an input and it's a\",\n","   'start': 300.72,\n","   'duration': 3.84},\n","  {'text': 'mathematical expression and the output',\n","   'start': 302.479,\n","   'duration': 3.841},\n","  {'text': 'are your predictions of your neural net',\n","   'start': 304.56,\n","   'duration': 3.6},\n","  {'text': \"or the loss function we'll see this in a\",\n","   'start': 306.32,\n","   'duration': 4.24},\n","  {'text': 'bit but basically neural networks just',\n","   'start': 308.16,\n","   'duration': 3.92},\n","  {'text': 'happen to be a certain class of',\n","   'start': 310.56,\n","   'duration': 3.199},\n","  {'text': 'mathematical expressions', 'start': 312.08, 'duration': 3.119},\n","  {'text': 'but back propagation is actually',\n","   'start': 313.759,\n","   'duration': 3.361},\n","  {'text': \"significantly more general it doesn't\",\n","   'start': 315.199,\n","   'duration': 3.601},\n","  {'text': 'actually care about neural networks at',\n","   'start': 317.12,\n","   'duration': 3.519},\n","  {'text': 'all it only tells us about arbitrary',\n","   'start': 318.8,\n","   'duration': 3.6},\n","  {'text': 'mathematical expressions and then we',\n","   'start': 320.639,\n","   'duration': 3.84},\n","  {'text': 'happen to use that machinery for',\n","   'start': 322.4,\n","   'duration': 4.4},\n","  {'text': 'training of neural networks now one more',\n","   'start': 324.479,\n","   'duration': 3.921},\n","  {'text': 'note i would like to make at this stage',\n","   'start': 326.8,\n","   'duration': 3.44},\n","  {'text': 'is that as you see here micrograd is a',\n","   'start': 328.4,\n","   'duration': 4.239},\n","  {'text': \"scalar valued auto grant engine so it's\",\n","   'start': 330.24,\n","   'duration': 4.0},\n","  {'text': 'working on the you know level of',\n","   'start': 332.639,\n","   'duration': 3.361},\n","  {'text': 'individual scalars like negative four',\n","   'start': 334.24,\n","   'duration': 3.519},\n","  {'text': \"and two and we're taking neural nets and\",\n","   'start': 336.0,\n","   'duration': 3.28},\n","  {'text': \"we're breaking them down all the way to\",\n","   'start': 337.759,\n","   'duration': 3.601},\n","  {'text': 'these atoms of individual scalars and',\n","   'start': 339.28,\n","   'duration': 3.84},\n","  {'text': \"all the little pluses and times and it's\",\n","   'start': 341.36,\n","   'duration': 4.24},\n","  {'text': 'just excessive and so obviously you',\n","   'start': 343.12,\n","   'duration': 4.0},\n","  {'text': 'would never be doing any of this in',\n","   'start': 345.6,\n","   'duration': 3.2},\n","  {'text': \"production it's really just put down for\",\n","   'start': 347.12,\n","   'duration': 3.68},\n","  {'text': 'pedagogical reasons because it allows us',\n","   'start': 348.8,\n","   'duration': 3.52},\n","  {'text': 'to not have to deal with these', 'start': 350.8, 'duration': 3.76},\n","  {'text': 'n-dimensional tensors that you would use',\n","   'start': 352.32,\n","   'duration': 4.64},\n","  {'text': 'in modern deep neural network library so',\n","   'start': 354.56,\n","   'duration': 4.4},\n","  {'text': 'this is really done so that you',\n","   'start': 356.96,\n","   'duration': 3.76},\n","  {'text': 'understand and refactor out back',\n","   'start': 358.96,\n","   'duration': 3.76},\n","  {'text': 'propagation and chain rule and',\n","   'start': 360.72,\n","   'duration': 4.24},\n","  {'text': 'understanding of neurologic training',\n","   'start': 362.72,\n","   'duration': 4.0},\n","  {'text': 'and then if you actually want to train',\n","   'start': 364.96,\n","   'duration': 3.12},\n","  {'text': 'bigger networks you have to be using',\n","   'start': 366.72,\n","   'duration': 3.12},\n","  {'text': 'these tensors but none of the math',\n","   'start': 368.08,\n","   'duration': 3.2},\n","  {'text': 'changes this is done purely for',\n","   'start': 369.84,\n","   'duration': 3.6},\n","  {'text': 'efficiency we are basically taking scale',\n","   'start': 371.28,\n","   'duration': 2.96},\n","  {'text': 'value', 'start': 373.44, 'duration': 2.56},\n","  {'text': \"all the scale values we're packaging\",\n","   'start': 374.24,\n","   'duration': 3.519},\n","  {'text': 'them up into tensors which are just',\n","   'start': 376.0,\n","   'duration': 4.08},\n","  {'text': 'arrays of these scalars and then because',\n","   'start': 377.759,\n","   'duration': 4.641},\n","  {'text': \"we have these large arrays we're making\",\n","   'start': 380.08,\n","   'duration': 4.32},\n","  {'text': 'operations on those large arrays that',\n","   'start': 382.4,\n","   'duration': 3.68},\n","  {'text': 'allows us to take advantage of the',\n","   'start': 384.4,\n","   'duration': 4.239},\n","  {'text': 'parallelism in a computer and all those',\n","   'start': 386.08,\n","   'duration': 4.399},\n","  {'text': 'operations can be done in parallel and',\n","   'start': 388.639,\n","   'duration': 3.761},\n","  {'text': 'then the whole thing runs faster but',\n","   'start': 390.479,\n","   'duration': 3.201},\n","  {'text': 'really none of the math changes and',\n","   'start': 392.4,\n","   'duration': 3.28},\n","  {'text': \"that's done purely for efficiency so i\",\n","   'start': 393.68,\n","   'duration': 3.2},\n","  {'text': \"don't think that it's pedagogically\",\n","   'start': 395.68,\n","   'duration': 2.959},\n","  {'text': 'useful to be dealing with tensors from',\n","   'start': 396.88,\n","   'duration': 3.84},\n","  {'text': \"scratch uh and i think and that's why i\",\n","   'start': 398.639,\n","   'duration': 3.921},\n","  {'text': 'fundamentally wrote micrograd because',\n","   'start': 400.72,\n","   'duration': 3.759},\n","  {'text': 'you can understand how things work uh at',\n","   'start': 402.56,\n","   'duration': 3.84},\n","  {'text': 'the fundamental level and then you can',\n","   'start': 404.479,\n","   'duration': 4.401},\n","  {'text': \"speed it up later okay so here's the fun\",\n","   'start': 406.4,\n","   'duration': 4.799},\n","  {'text': 'part my claim is that micrograd is what',\n","   'start': 408.88,\n","   'duration': 3.92},\n","  {'text': 'you need to train your networks and',\n","   'start': 411.199,\n","   'duration': 3.761},\n","  {'text': 'everything else is just efficiency so',\n","   'start': 412.8,\n","   'duration': 3.519},\n","  {'text': \"you'd think that micrograd would be a\",\n","   'start': 414.96,\n","   'duration': 3.6},\n","  {'text': 'very complex piece of code and that',\n","   'start': 416.319,\n","   'duration': 4.72},\n","  {'text': 'turns out to not be the case', 'start': 418.56, 'duration': 4.639},\n","  {'text': 'so if we just go to micrograd', 'start': 421.039, 'duration': 4.0},\n","  {'text': \"and you'll see that there's only two\",\n","   'start': 423.199,\n","   'duration': 4.161},\n","  {'text': 'files here in micrograd this is the',\n","   'start': 425.039,\n","   'duration': 4.081},\n","  {'text': \"actual engine it doesn't know anything\",\n","   'start': 427.36,\n","   'duration': 3.52},\n","  {'text': 'about neural nuts and this is the entire',\n","   'start': 429.12,\n","   'duration': 3.6},\n","  {'text': 'neural nets library', 'start': 430.88, 'duration': 6.319},\n","  {'text': 'on top of micrograd so engine and nn.pi',\n","   'start': 432.72,\n","   'duration': 7.12},\n","  {'text': 'so the actual backpropagation autograd',\n","   'start': 437.199,\n","   'duration': 4.0},\n","  {'text': 'engine', 'start': 439.84, 'duration': 2.56},\n","  {'text': 'that gives you the power of neural',\n","   'start': 441.199,\n","   'duration': 4.881},\n","  {'text': 'networks is literally', 'start': 442.4, 'duration': 6.0},\n","  {'text': '100 lines of code of like very simple',\n","   'start': 446.08,\n","   'duration': 3.92},\n","  {'text': 'python', 'start': 448.4, 'duration': 2.72},\n","  {'text': \"which we'll understand by the end of\",\n","   'start': 450.0,\n","   'duration': 2.08},\n","  {'text': 'this lecture', 'start': 451.12, 'duration': 2.72},\n","  {'text': 'and then nn.pi', 'start': 452.08, 'duration': 3.6},\n","  {'text': 'this neural network library built on top',\n","   'start': 453.84,\n","   'duration': 3.919},\n","  {'text': 'of the autograd engine', 'start': 455.68, 'duration': 4.88},\n","  {'text': \"um is like a joke it's like\", 'start': 457.759, 'duration': 4.961},\n","  {'text': 'we have to define what is a neuron and',\n","   'start': 460.56,\n","   'duration': 3.44},\n","  {'text': 'then we have to define what is the layer',\n","   'start': 462.72,\n","   'duration': 3.36},\n","  {'text': 'of neurons and then we define what is a',\n","   'start': 464.0,\n","   'duration': 3.84},\n","  {'text': 'multi-layer perceptron which is just a',\n","   'start': 466.08,\n","   'duration': 4.32},\n","  {'text': 'sequence of layers of neurons and so',\n","   'start': 467.84,\n","   'duration': 4.16},\n","  {'text': \"it's just a total joke\", 'start': 470.4, 'duration': 3.04},\n","  {'text': 'so basically', 'start': 472.0, 'duration': 3.68},\n","  {'text': \"there's a lot of power that comes from\",\n","   'start': 473.44,\n","   'duration': 4.159},\n","  {'text': 'only 150 lines of code', 'start': 475.68, 'duration': 3.44},\n","  {'text': \"and that's all you need to understand to\",\n","   'start': 477.599,\n","   'duration': 3.28},\n","  {'text': 'understand neural network training and',\n","   'start': 479.12,\n","   'duration': 3.6},\n","  {'text': 'everything else is just efficiency and',\n","   'start': 480.879,\n","   'duration': 4.801},\n","  {'text': \"of course there's a lot to efficiency\",\n","   'start': 482.72,\n","   'duration': 4.4},\n","  {'text': \"but fundamentally that's all that's\",\n","   'start': 485.68,\n","   'duration': 3.44},\n","  {'text': \"happening okay so now let's dive right\",\n","   'start': 487.12,\n","   'duration': 4.16},\n","  {'text': 'in and implement micrograph step by step',\n","   'start': 489.12,\n","   'duration': 3.28},\n","  {'text': \"the first thing i'd like to do is i'd\",\n","   'start': 491.28,\n","   'duration': 2.479},\n","  {'text': 'like to make sure that you have a very',\n","   'start': 492.4,\n","   'duration': 3.84},\n","  {'text': 'good understanding intuitively of what a',\n","   'start': 493.759,\n","   'duration': 4.481},\n","  {'text': 'derivative is and exactly what',\n","   'start': 496.24,\n","   'duration': 4.32},\n","  {'text': \"information it gives you so let's start\",\n","   'start': 498.24,\n","   'duration': 4.079},\n","  {'text': 'with some basic imports that i copy',\n","   'start': 500.56,\n","   'duration': 4.72},\n","  {'text': 'paste in every jupiter notebook always',\n","   'start': 502.319,\n","   'duration': 5.041},\n","  {'text': \"and let's define a function a scalar\",\n","   'start': 505.28,\n","   'duration': 3.68},\n","  {'text': 'valued function', 'start': 507.36, 'duration': 2.88},\n","  {'text': 'f of x', 'start': 508.96, 'duration': 2.4},\n","  {'text': 'as follows', 'start': 510.24, 'duration': 2.96},\n","  {'text': 'so i just make this up randomly i just',\n","   'start': 511.36,\n","   'duration': 3.2},\n","  {'text': 'want to scale a valid function that',\n","   'start': 513.2,\n","   'duration': 3.519},\n","  {'text': 'takes a single scalar x and returns a',\n","   'start': 514.56,\n","   'duration': 4.0},\n","  {'text': 'single scalar y', 'start': 516.719, 'duration': 3.361},\n","  {'text': 'and we can call this function of course',\n","   'start': 518.56,\n","   'duration': 4.24},\n","  {'text': 'so we can pass in say 3.0 and get 20',\n","   'start': 520.08,\n","   'duration': 3.839},\n","  {'text': 'back', 'start': 522.8, 'duration': 2.8},\n","  {'text': 'now we can also plot this function to',\n","   'start': 523.919,\n","   'duration': 3.521},\n","  {'text': 'get a sense of its shape you can tell',\n","   'start': 525.6,\n","   'duration': 3.28},\n","  {'text': 'from the mathematical expression that',\n","   'start': 527.44,\n","   'duration': 3.28},\n","  {'text': \"this is probably a parabola it's a\",\n","   'start': 528.88,\n","   'duration': 2.959},\n","  {'text': 'quadratic', 'start': 530.72, 'duration': 5.679},\n","  {'text': 'and so if we just uh create a set of um',\n","   'start': 531.839,\n","   'duration': 5.841},\n","  {'text': 'um', 'start': 536.399, 'duration': 3.12},\n","  {'text': 'scale values that we can feed in using',\n","   'start': 537.68,\n","   'duration': 3.44},\n","  {'text': 'for example a range from negative five',\n","   'start': 539.519,\n","   'duration': 4.401},\n","  {'text': 'to five in steps of 0.25', 'start': 541.12, 'duration': 5.52},\n","  {'text': 'so this is so axis is just from negative',\n","   'start': 543.92,\n","   'duration': 7.44},\n","  {'text': '5 to 5 not including 5 in steps of 0.25',\n","   'start': 546.64,\n","   'duration': 6.08},\n","  {'text': 'and we can actually call this function',\n","   'start': 551.36,\n","   'duration': 3.28},\n","  {'text': 'on this numpy array as well so we get a',\n","   'start': 552.72,\n","   'duration': 4.96},\n","  {'text': \"set of y's if we call f on axis\",\n","   'start': 554.64,\n","   'duration': 5.759},\n","  {'text': \"and these y's are basically\", 'start': 557.68, 'duration': 5.839},\n","  {'text': 'also applying a function on every one of',\n","   'start': 560.399,\n","   'duration': 4.961},\n","  {'text': 'these elements independently',\n","   'start': 563.519,\n","   'duration': 4.481},\n","  {'text': 'and we can plot this using matplotlib so',\n","   'start': 565.36,\n","   'duration': 5.84},\n","  {'text': \"plt.plot x's and y's and we get a nice\",\n","   'start': 568.0,\n","   'duration': 5.76},\n","  {'text': 'parabola so previously here we fed in',\n","   'start': 571.2,\n","   'duration': 5.36},\n","  {'text': '3.0 somewhere here and we received 20',\n","   'start': 573.76,\n","   'duration': 5.28},\n","  {'text': 'back which is here the y coordinate so',\n","   'start': 576.56,\n","   'duration': 4.08},\n","  {'text': \"now i'd like to think through\",\n","   'start': 579.04,\n","   'duration': 3.359},\n","  {'text': 'what is the derivative', 'start': 580.64, 'duration': 3.68},\n","  {'text': 'of this function at any single input',\n","   'start': 582.399,\n","   'duration': 3.12},\n","  {'text': 'point x', 'start': 584.32, 'duration': 2.72},\n","  {'text': 'right so what is the derivative at',\n","   'start': 585.519,\n","   'duration': 4.32},\n","  {'text': 'different points x of this function now',\n","   'start': 587.04,\n","   'duration': 4.16},\n","  {'text': 'if you remember back to your calculus',\n","   'start': 589.839,\n","   'duration': 2.401},\n","  {'text': \"class you've probably derived\", 'start': 591.2, 'duration': 3.44},\n","  {'text': 'derivatives so we take this mathematical',\n","   'start': 592.24,\n","   'duration': 4.96},\n","  {'text': 'expression 3x squared minus 4x plus 5',\n","   'start': 594.64,\n","   'duration': 3.759},\n","  {'text': 'and you would write out on a piece of',\n","   'start': 597.2,\n","   'duration': 2.72},\n","  {'text': 'paper and you would you know apply the',\n","   'start': 598.399,\n","   'duration': 2.961},\n","  {'text': 'product rule and all the other rules and',\n","   'start': 599.92,\n","   'duration': 3.52},\n","  {'text': 'derive the mathematical expression of',\n","   'start': 601.36,\n","   'duration': 4.08},\n","  {'text': 'the great derivative of the original',\n","   'start': 603.44,\n","   'duration': 3.28},\n","  {'text': 'function and then you could plug in',\n","   'start': 605.44,\n","   'duration': 2.56},\n","  {'text': 'different texts and see what the',\n","   'start': 606.72,\n","   'duration': 3.04},\n","  {'text': 'derivative is', 'start': 608.0, 'duration': 3.519},\n","  {'text': \"we're not going to actually do that\",\n","   'start': 609.76,\n","   'duration': 3.92},\n","  {'text': 'because no one in neural networks',\n","   'start': 611.519,\n","   'duration': 3.841},\n","  {'text': 'actually writes out the expression for',\n","   'start': 613.68,\n","   'duration': 2.96},\n","  {'text': 'the neural net it would be a massive',\n","   'start': 615.36,\n","   'duration': 3.52},\n","  {'text': 'expression um it would be you know',\n","   'start': 616.64,\n","   'duration': 3.84},\n","  {'text': 'thousands tens of thousands of terms no',\n","   'start': 618.88,\n","   'duration': 3.76},\n","  {'text': 'one actually derives the derivative of',\n","   'start': 620.48,\n","   'duration': 3.919},\n","  {'text': \"course and so we're not going to take\",\n","   'start': 622.64,\n","   'duration': 3.52},\n","  {'text': 'this kind of like a symbolic approach',\n","   'start': 624.399,\n","   'duration': 2.961},\n","  {'text': \"instead what i'd like to do is i'd like\",\n","   'start': 626.16,\n","   'duration': 3.119},\n","  {'text': 'to look at the definition of derivative',\n","   'start': 627.36,\n","   'duration': 3.039},\n","  {'text': 'and just make sure that we really',\n","   'start': 629.279,\n","   'duration': 3.041},\n","  {'text': 'understand what derivative is measuring',\n","   'start': 630.399,\n","   'duration': 4.401},\n","  {'text': \"what it's telling you about the function\",\n","   'start': 632.32,\n","   'duration': 6.32},\n","  {'text': 'and so if we just look up derivative',\n","   'start': 634.8,\n","   'duration': 3.84},\n","  {'text': 'we see that', 'start': 642.32, 'duration': 2.32},\n","  {'text': 'okay so this is not a very good',\n","   'start': 643.519,\n","   'duration': 2.56},\n","  {'text': 'definition of derivative this is a',\n","   'start': 644.64,\n","   'duration': 2.639},\n","  {'text': 'definition of what it means to be',\n","   'start': 646.079,\n","   'duration': 2.401},\n","  {'text': 'differentiable', 'start': 647.279, 'duration': 2.881},\n","  {'text': 'but if you remember from your calculus',\n","   'start': 648.48,\n","   'duration': 4.08},\n","  {'text': 'it is the limit as h goes to zero of f',\n","   'start': 650.16,\n","   'duration': 5.76},\n","  {'text': 'of x plus h minus f of x over h so',\n","   'start': 652.56,\n","   'duration': 5.839},\n","  {'text': \"basically what it's saying is if you\",\n","   'start': 655.92,\n","   'duration': 5.039},\n","  {'text': \"slightly bump up you're at some point x\",\n","   'start': 658.399,\n","   'duration': 4.481},\n","  {'text': \"that you're interested in or a and if\",\n","   'start': 660.959,\n","   'duration': 3.601},\n","  {'text': 'you slightly bump up', 'start': 662.88, 'duration': 3.199},\n","  {'text': 'you know you slightly increase it by',\n","   'start': 664.56,\n","   'duration': 3.519},\n","  {'text': 'small number h', 'start': 666.079, 'duration': 3.681},\n","  {'text': 'how does the function respond with what',\n","   'start': 668.079,\n","   'duration': 3.361},\n","  {'text': 'sensitivity does it respond what is the',\n","   'start': 669.76,\n","   'duration': 3.84},\n","  {'text': 'slope at that point does the function go',\n","   'start': 671.44,\n","   'duration': 4.88},\n","  {'text': 'up or does it go down and by how much',\n","   'start': 673.6,\n","   'duration': 4.4},\n","  {'text': \"and that's the slope of that function\",\n","   'start': 676.32,\n","   'duration': 2.4},\n","  {'text': 'the', 'start': 678.0, 'duration': 3.76},\n","  {'text': 'the slope of that response at that point',\n","   'start': 678.72,\n","   'duration': 5.2},\n","  {'text': 'and so we can basically evaluate',\n","   'start': 681.76,\n","   'duration': 4.56},\n","  {'text': 'the derivative here numerically by',\n","   'start': 683.92,\n","   'duration': 4.08},\n","  {'text': 'taking a very small h of course the',\n","   'start': 686.32,\n","   'duration': 3.84},\n","  {'text': 'definition would ask us to take h to',\n","   'start': 688.0,\n","   'duration': 3.76},\n","  {'text': \"zero we're just going to pick a very\",\n","   'start': 690.16,\n","   'duration': 3.84},\n","  {'text': 'small h 0.001', 'start': 691.76, 'duration': 3.36},\n","  {'text': \"and let's say we're interested in point\",\n","   'start': 694.0,\n","   'duration': 3.76},\n","  {'text': '3.0 so we can look at f of x of course',\n","   'start': 695.12,\n","   'duration': 3.839},\n","  {'text': 'as 20', 'start': 697.76, 'duration': 3.199},\n","  {'text': 'and now f of x plus h', 'start': 698.959, 'duration': 3.841},\n","  {'text': 'so if we slightly nudge x in a positive',\n","   'start': 700.959,\n","   'duration': 3.601},\n","  {'text': 'direction how is the function going to',\n","   'start': 702.8,\n","   'duration': 2.8},\n","  {'text': 'respond', 'start': 704.56, 'duration': 3.12},\n","  {'text': 'and just looking at this do you expect',\n","   'start': 705.6,\n","   'duration': 3.6},\n","  {'text': 'do you expect f of x plus h to be',\n","   'start': 707.68,\n","   'duration': 4.24},\n","  {'text': 'slightly greater than 20 or do you',\n","   'start': 709.2,\n","   'duration': 5.439},\n","  {'text': 'expect to be slightly lower than 20',\n","   'start': 711.92,\n","   'duration': 5.12},\n","  {'text': 'and since this 3 is here and this is 20',\n","   'start': 714.639,\n","   'duration': 4.401},\n","  {'text': 'if we slightly go positively the',\n","   'start': 717.04,\n","   'duration': 4.32},\n","  {'text': 'function will respond positively so',\n","   'start': 719.04,\n","   'duration': 4.0},\n","  {'text': \"you'd expect this to be slightly greater\",\n","   'start': 721.36,\n","   'duration': 4.0},\n","  {'text': \"than 20. and now by how much it's\",\n","   'start': 723.04,\n","   'duration': 3.44},\n","  {'text': 'telling you the', 'start': 725.36, 'duration': 2.479},\n","  {'text': 'sort of the', 'start': 726.48, 'duration': 3.44},\n","  {'text': 'the strength of that slope right the the',\n","   'start': 727.839,\n","   'duration': 4.961},\n","  {'text': 'size of the slope so f of x plus h minus',\n","   'start': 729.92,\n","   'duration': 4.88},\n","  {'text': 'f of x this is how much the function',\n","   'start': 732.8,\n","   'duration': 3.2},\n","  {'text': 'responded', 'start': 734.8, 'duration': 3.12},\n","  {'text': 'in the positive direction and we have to',\n","   'start': 736.0,\n","   'duration': 3.92},\n","  {'text': 'normalize by the', 'start': 737.92, 'duration': 4.4},\n","  {'text': 'run so we have the rise over run to get',\n","   'start': 739.92,\n","   'duration': 4.88},\n","  {'text': 'the slope so this of course is just a',\n","   'start': 742.32,\n","   'duration': 4.48},\n","  {'text': 'numerical approximation of the slope',\n","   'start': 744.8,\n","   'duration': 4.08},\n","  {'text': 'because we have to make age very very',\n","   'start': 746.8,\n","   'duration': 6.0},\n","  {'text': 'small to converge to the exact amount',\n","   'start': 748.88,\n","   'duration': 6.639},\n","  {'text': \"now if i'm doing too many zeros\", 'start': 752.8, 'duration': 4.0},\n","  {'text': 'at some point', 'start': 755.519, 'duration': 2.801},\n","  {'text': \"i'm gonna get an incorrect answer\",\n","   'start': 756.8,\n","   'duration': 2.8},\n","  {'text': \"because we're using floating point\",\n","   'start': 758.32,\n","   'duration': 3.519},\n","  {'text': 'arithmetic and the representations of',\n","   'start': 759.6,\n","   'duration': 4.16},\n","  {'text': 'all these numbers in computer memory is',\n","   'start': 761.839,\n","   'duration': 3.761},\n","  {'text': 'finite and at some point we get into',\n","   'start': 763.76,\n","   'duration': 2.639},\n","  {'text': 'trouble', 'start': 765.6, 'duration': 2.16},\n","  {'text': 'so we can converse towards the right',\n","   'start': 766.399,\n","   'duration': 4.0},\n","  {'text': 'answer with this approach', 'start': 767.76, 'duration': 6.8},\n","  {'text': 'but basically um at 3 the slope is 14.',\n","   'start': 770.399,\n","   'duration': 5.68},\n","  {'text': 'and you can see that by taking 3x',\n","   'start': 774.56,\n","   'duration': 3.76},\n","  {'text': 'squared minus 4x plus 5 and', 'start': 776.079, 'duration': 4.401},\n","  {'text': 'differentiating it in our head',\n","   'start': 778.32,\n","   'duration': 4.319},\n","  {'text': 'so 3x squared would be', 'start': 780.48, 'duration': 4.4},\n","  {'text': '6 x minus 4', 'start': 782.639, 'duration': 4.64},\n","  {'text': \"and then we plug in x equals 3 so that's\",\n","   'start': 784.88,\n","   'duration': 6.0},\n","  {'text': '18 minus 4 is 14. so this is correct',\n","   'start': 787.279,\n","   'duration': 4.8},\n","  {'text': \"so that's\", 'start': 790.88, 'duration': 4.88},\n","  {'text': 'at 3. now how about the slope at say',\n","   'start': 792.079,\n","   'duration': 5.281},\n","  {'text': 'negative 3', 'start': 795.76, 'duration': 3.759},\n","  {'text': 'would you expect would you expect for',\n","   'start': 797.36,\n","   'duration': 3.12},\n","  {'text': 'the slope', 'start': 799.519, 'duration': 2.801},\n","  {'text': 'now telling the exact value is really',\n","   'start': 800.48,\n","   'duration': 4.479},\n","  {'text': 'hard but what is the sign of that slope',\n","   'start': 802.32,\n","   'duration': 4.48},\n","  {'text': 'so at negative three', 'start': 804.959, 'duration': 3.44},\n","  {'text': 'if we slightly go in the positive',\n","   'start': 806.8,\n","   'duration': 3.92},\n","  {'text': 'direction at x the function would',\n","   'start': 808.399,\n","   'duration': 4.161},\n","  {'text': 'actually go down and so that tells you',\n","   'start': 810.72,\n","   'duration': 3.2},\n","  {'text': 'that the slope would be negative so',\n","   'start': 812.56,\n","   'duration': 4.16},\n","  {'text': \"we'll get a slight number below\",\n","   'start': 813.92,\n","   'duration': 5.2},\n","  {'text': 'below 20. and so if we take the slope we',\n","   'start': 816.72,\n","   'duration': 4.16},\n","  {'text': 'expect something negative', 'start': 819.12, 'duration': 4.719},\n","  {'text': 'negative 22. okay', 'start': 820.88, 'duration': 4.639},\n","  {'text': 'and at some point here of course the',\n","   'start': 823.839,\n","   'duration': 3.761},\n","  {'text': 'slope would be zero now for this',\n","   'start': 825.519,\n","   'duration': 3.361},\n","  {'text': 'specific function i looked it up',\n","   'start': 827.6,\n","   'duration': 3.679},\n","  {'text': \"previously and it's at point two over\",\n","   'start': 828.88,\n","   'duration': 3.28},\n","  {'text': 'three', 'start': 831.279, 'duration': 3.041},\n","  {'text': 'so at roughly two over three', 'start': 832.16, 'duration': 3.76},\n","  {'text': \"uh that's somewhere here\", 'start': 834.32, 'duration': 2.8},\n","  {'text': 'um', 'start': 835.92, 'duration': 3.279},\n","  {'text': 'this derivative be zero', 'start': 837.12, 'duration': 6.32},\n","  {'text': 'so basically at that precise point',\n","   'start': 839.199,\n","   'duration': 5.041},\n","  {'text': 'yeah', 'start': 843.44, 'duration': 2.88},\n","  {'text': 'at that precise point if we nudge in a',\n","   'start': 844.24,\n","   'duration': 3.52},\n","  {'text': \"positive direction the function doesn't\",\n","   'start': 846.32,\n","   'duration': 3.519},\n","  {'text': 'respond this stays the same almost and',\n","   'start': 847.76,\n","   'duration': 4.079},\n","  {'text': \"so that's why the slope is zero okay now\",\n","   'start': 849.839,\n","   'duration': 4.481},\n","  {'text': \"let's look at a bit more complex case\",\n","   'start': 851.839,\n","   'duration': 4.0},\n","  {'text': \"so we're going to start you know\",\n","   'start': 854.32,\n","   'duration': 3.68},\n","  {'text': 'complexifying a bit so now we have a',\n","   'start': 855.839,\n","   'duration': 3.841},\n","  {'text': 'function', 'start': 858.0, 'duration': 2.959},\n","  {'text': 'here', 'start': 859.68, 'duration': 2.959},\n","  {'text': 'with output variable d', 'start': 860.959, 'duration': 3.201},\n","  {'text': 'that is a function of three scalar',\n","   'start': 862.639,\n","   'duration': 3.521},\n","  {'text': 'inputs a b and c', 'start': 864.16, 'duration': 4.32},\n","  {'text': 'so a b and c are some specific values',\n","   'start': 866.16,\n","   'duration': 4.479},\n","  {'text': 'three inputs into our expression graph',\n","   'start': 868.48,\n","   'duration': 4.32},\n","  {'text': 'and a single output d', 'start': 870.639, 'duration': 5.681},\n","  {'text': 'and so if we just print d we get four',\n","   'start': 872.8,\n","   'duration': 5.36},\n","  {'text': \"and now what i have to do is i'd like to\",\n","   'start': 876.32,\n","   'duration': 3.92},\n","  {'text': 'again look at the derivatives of d with',\n","   'start': 878.16,\n","   'duration': 4.32},\n","  {'text': 'respect to a b and c', 'start': 880.24, 'duration': 4.64},\n","  {'text': 'and uh think through uh again just the',\n","   'start': 882.48,\n","   'duration': 3.76},\n","  {'text': 'intuition of what this derivative is',\n","   'start': 884.88,\n","   'duration': 2.639},\n","  {'text': 'telling us', 'start': 886.24, 'duration': 3.599},\n","  {'text': 'so in order to evaluate this derivative',\n","   'start': 887.519,\n","   'duration': 4.56},\n","  {'text': \"we're going to get a bit hacky here\",\n","   'start': 889.839,\n","   'duration': 4.081},\n","  {'text': \"we're going to again have a very small\",\n","   'start': 892.079,\n","   'duration': 3.281},\n","  {'text': 'value of h', 'start': 893.92, 'duration': 3.44},\n","  {'text': \"and then we're going to fix the inputs\",\n","   'start': 895.36,\n","   'duration': 3.2},\n","  {'text': 'at some', 'start': 897.36, 'duration': 2.96},\n","  {'text': \"values that we're interested in\",\n","   'start': 898.56,\n","   'duration': 4.399},\n","  {'text': 'so these are the this is the point abc',\n","   'start': 900.32,\n","   'duration': 3.92},\n","  {'text': \"at which we're going to be evaluating\",\n","   'start': 902.959,\n","   'duration': 2.481},\n","  {'text': 'the the', 'start': 904.24, 'duration': 3.52},\n","  {'text': 'derivative of d with respect to all a b',\n","   'start': 905.44,\n","   'duration': 4.32},\n","  {'text': 'and c at that point', 'start': 907.76, 'duration': 3.6},\n","  {'text': 'so there are the inputs and now we have',\n","   'start': 909.76,\n","   'duration': 4.0},\n","  {'text': 'd1 is that expression', 'start': 911.36, 'duration': 3.68},\n","  {'text': \"and then we're going to for example look\",\n","   'start': 913.76,\n","   'duration': 3.36},\n","  {'text': 'at the derivative of d with respect to a',\n","   'start': 915.04,\n","   'duration': 4.88},\n","  {'text': \"so we'll take a and we'll bump it by h\",\n","   'start': 917.12,\n","   'duration': 4.88},\n","  {'text': \"and then we'll get d2 to be the exact\",\n","   'start': 919.92,\n","   'duration': 3.919},\n","  {'text': 'same function', 'start': 922.0, 'duration': 4.8},\n","  {'text': \"and now we're going to print um\",\n","   'start': 923.839,\n","   'duration': 4.881},\n","  {'text': 'you know f1', 'start': 926.8, 'duration': 4.32},\n","  {'text': 'd1 is d1', 'start': 928.72, 'duration': 4.16},\n","  {'text': 'd2 is d2', 'start': 931.12, 'duration': 4.079},\n","  {'text': 'and print slope', 'start': 932.88, 'duration': 4.8},\n","  {'text': 'so the derivative or slope', 'start': 935.199, 'duration': 4.481},\n","  {'text': 'here will be um', 'start': 937.68, 'duration': 3.519},\n","  {'text': 'of course', 'start': 939.68, 'duration': 2.399},\n","  {'text': 'd2', 'start': 941.199, 'duration': 3.2},\n","  {'text': 'minus d1 divide h', 'start': 942.079, 'duration': 5.361},\n","  {'text': 'so d2 minus d1 is how much the function',\n","   'start': 944.399,\n","   'duration': 4.321},\n","  {'text': 'increased', 'start': 947.44, 'duration': 3.28},\n","  {'text': 'uh when we bumped', 'start': 948.72, 'duration': 3.2},\n","  {'text': 'the uh', 'start': 950.72, 'duration': 2.72},\n","  {'text': \"the specific input that we're interested\",\n","   'start': 951.92,\n","   'duration': 3.919},\n","  {'text': 'in by a tiny amount', 'start': 953.44, 'duration': 3.28},\n","  {'text': 'and', 'start': 955.839, 'duration': 3.36},\n","  {'text': 'this is then normalized by h', 'start': 956.72, 'duration': 5.359},\n","  {'text': 'to get the slope', 'start': 959.199, 'duration': 2.88},\n","  {'text': 'so', 'start': 962.8, 'duration': 2.32},\n","  {'text': 'um', 'start': 963.6, 'duration': 2.72},\n","  {'text': 'yeah', 'start': 965.12, 'duration': 3.76},\n","  {'text': \"so this so if i just run this we're\",\n","   'start': 966.32,\n","   'duration': 4.079},\n","  {'text': 'going to print', 'start': 968.88, 'duration': 3.36},\n","  {'text': 'd1', 'start': 970.399, 'duration': 5.041},\n","  {'text': 'which we know is four', 'start': 972.24, 'duration': 6.24},\n","  {'text': 'now d2 will be bumped a will be bumped',\n","   'start': 975.44,\n","   'duration': 4.88},\n","  {'text': 'by h', 'start': 978.48, 'duration': 4.0},\n","  {'text': \"so let's just think through\", 'start': 980.32, 'duration': 5.84},\n","  {'text': 'a little bit uh what d2 will be uh',\n","   'start': 982.48,\n","   'duration': 5.12},\n","  {'text': 'printed out here', 'start': 986.16, 'duration': 3.119},\n","  {'text': 'in particular', 'start': 987.6, 'duration': 3.52},\n","  {'text': 'd1 will be four', 'start': 989.279, 'duration': 4.24},\n","  {'text': 'will d2 be a number slightly greater',\n","   'start': 991.12,\n","   'duration': 4.56},\n","  {'text': 'than four or slightly lower than four',\n","   'start': 993.519,\n","   'duration': 4.081},\n","  {'text': \"and that's going to tell us the sl the\",\n","   'start': 995.68,\n","   'duration': 4.48},\n","  {'text': 'the sign of the derivative', 'start': 997.6, 'duration': 4.799},\n","  {'text': 'so', 'start': 1000.16, 'duration': 2.239},\n","  {'text': \"we're bumping a by h\", 'start': 1002.56, 'duration': 6.0},\n","  {'text': 'b as minus three c is ten', 'start': 1005.44, 'duration': 4.48},\n","  {'text': 'so you can just intuitively think',\n","   'start': 1008.56,\n","   'duration': 2.56},\n","  {'text': \"through this derivative and what it's\",\n","   'start': 1009.92,\n","   'duration': 4.88},\n","  {'text': 'doing a will be slightly more positive',\n","   'start': 1011.12,\n","   'duration': 6.399},\n","  {'text': 'and but b is a negative number',\n","   'start': 1014.8,\n","   'duration': 5.599},\n","  {'text': 'so if a is slightly more positive',\n","   'start': 1017.519,\n","   'duration': 5.68},\n","  {'text': 'because b is negative three', 'start': 1020.399, 'duration': 5.68},\n","  {'text': \"we're actually going to be adding less\",\n","   'start': 1023.199,\n","   'duration': 4.961},\n","  {'text': 'to d', 'start': 1026.079, 'duration': 4.561},\n","  {'text': \"so you'd actually expect that the value\",\n","   'start': 1028.16,\n","   'duration': 5.6},\n","  {'text': 'of the function will go down', 'start': 1030.64, 'duration': 5.84},\n","  {'text': \"so let's just see this\", 'start': 1033.76, 'duration': 4.64},\n","  {'text': 'yeah and so we went from 4', 'start': 1036.48, 'duration': 4.319},\n","  {'text': 'to 3.9996', 'start': 1038.4, 'duration': 3.76},\n","  {'text': 'and that tells you that the slope will',\n","   'start': 1040.799,\n","   'duration': 2.481},\n","  {'text': 'be negative', 'start': 1042.16, 'duration': 2.32},\n","  {'text': 'and then', 'start': 1043.28, 'duration': 3.12},\n","  {'text': 'uh will be a negative number',\n","   'start': 1044.48,\n","   'duration': 3.439},\n","  {'text': 'because we went down', 'start': 1046.4, 'duration': 2.639},\n","  {'text': 'and then', 'start': 1047.919, 'duration': 3.12},\n","  {'text': 'the exact number of slope will be',\n","   'start': 1049.039,\n","   'duration': 4.721},\n","  {'text': 'exact amount of slope is negative 3.',\n","   'start': 1051.039,\n","   'duration': 4.081},\n","  {'text': 'and you can also convince yourself that',\n","   'start': 1053.76,\n","   'duration': 3.2},\n","  {'text': 'negative 3 is the right answer',\n","   'start': 1055.12,\n","   'duration': 3.919},\n","  {'text': 'mathematically and analytically because',\n","   'start': 1056.96,\n","   'duration': 4.64},\n","  {'text': 'if you have a times b plus c and you are',\n","   'start': 1059.039,\n","   'duration': 4.801},\n","  {'text': 'you know you have calculus then',\n","   'start': 1061.6,\n","   'duration': 4.4},\n","  {'text': 'differentiating a times b plus c with',\n","   'start': 1063.84,\n","   'duration': 4.64},\n","  {'text': 'respect to a gives you just b',\n","   'start': 1066.0,\n","   'duration': 4.799},\n","  {'text': 'and indeed the value of b is negative 3',\n","   'start': 1068.48,\n","   'duration': 4.319},\n","  {'text': 'which is the derivative that we have so',\n","   'start': 1070.799,\n","   'duration': 4.161},\n","  {'text': \"you can tell that that's correct\",\n","   'start': 1072.799,\n","   'duration': 4.961},\n","  {'text': 'so now if we do this with b', 'start': 1074.96, 'duration': 4.88},\n","  {'text': 'so if we bump b by a little bit in a',\n","   'start': 1077.76,\n","   'duration': 4.48},\n","  {'text': \"positive direction we'd get different\",\n","   'start': 1079.84,\n","   'duration': 4.719},\n","  {'text': 'slopes so what is the influence of b on',\n","   'start': 1082.24,\n","   'duration': 4.08},\n","  {'text': 'the output d', 'start': 1084.559, 'duration': 3.681},\n","  {'text': 'so if we bump b by a tiny amount in a',\n","   'start': 1086.32,\n","   'duration': 3.84},\n","  {'text': 'positive direction then because a is',\n","   'start': 1088.24,\n","   'duration': 3.36},\n","  {'text': 'positive', 'start': 1090.16, 'duration': 3.6},\n","  {'text': \"we'll be adding more to d\", 'start': 1091.6, 'duration': 3.12},\n","  {'text': 'right', 'start': 1093.76, 'duration': 3.279},\n","  {'text': 'so um and now what is the what is the',\n","   'start': 1094.72,\n","   'duration': 4.079},\n","  {'text': 'sensitivity what is the slope of that',\n","   'start': 1097.039,\n","   'duration': 2.801},\n","  {'text': 'addition', 'start': 1098.799, 'duration': 2.721},\n","  {'text': 'and it might not surprise you that this',\n","   'start': 1099.84,\n","   'duration': 3.12},\n","  {'text': 'should be', 'start': 1101.52, 'duration': 2.72},\n","  {'text': '2', 'start': 1102.96, 'duration': 4.719},\n","  {'text': 'and y is a 2 because d of d', 'start': 1104.24, 'duration': 5.92},\n","  {'text': 'by db differentiating with respect to b',\n","   'start': 1107.679,\n","   'duration': 4.24},\n","  {'text': 'would be would give us a', 'start': 1110.16, 'duration': 3.92},\n","  {'text': \"and the value of a is two so that's also\",\n","   'start': 1111.919,\n","   'duration': 3.521},\n","  {'text': 'working well', 'start': 1114.08, 'duration': 3.36},\n","  {'text': 'and then if c gets bumped a tiny amount',\n","   'start': 1115.44,\n","   'duration': 3.119},\n","  {'text': 'in h', 'start': 1117.44, 'duration': 2.479},\n","  {'text': 'by h', 'start': 1118.559, 'duration': 3.281},\n","  {'text': 'then of course a times b is unaffected',\n","   'start': 1119.919,\n","   'duration': 4.321},\n","  {'text': 'and now c becomes slightly bit higher',\n","   'start': 1121.84,\n","   'duration': 4.079},\n","  {'text': 'what does that do to the function it',\n","   'start': 1124.24,\n","   'duration': 3.04},\n","  {'text': 'makes it slightly bit higher because',\n","   'start': 1125.919,\n","   'duration': 2.88},\n","  {'text': \"we're simply adding c\", 'start': 1127.28, 'duration': 3.2},\n","  {'text': 'and it makes it slightly bit higher by',\n","   'start': 1128.799,\n","   'duration': 4.321},\n","  {'text': 'the exact same amount that we added to c',\n","   'start': 1130.48,\n","   'duration': 4.72},\n","  {'text': 'and so that tells you that the slope is',\n","   'start': 1133.12,\n","   'duration': 3.439},\n","  {'text': 'one', 'start': 1135.2, 'duration': 4.0},\n","  {'text': 'that will be the', 'start': 1136.559, 'duration': 4.48},\n","  {'text': 'the rate at which', 'start': 1139.2, 'duration': 4.96},\n","  {'text': 'd will increase as we scale',\n","   'start': 1141.039,\n","   'duration': 4.081},\n","  {'text': 'c', 'start': 1144.16, 'duration': 2.639},\n","  {'text': 'okay so we now have some intuitive sense',\n","   'start': 1145.12,\n","   'duration': 3.12},\n","  {'text': 'of what this derivative is telling you',\n","   'start': 1146.799,\n","   'duration': 3.201},\n","  {'text': \"about the function and we'd like to move\",\n","   'start': 1148.24,\n","   'duration': 3.36},\n","  {'text': 'to neural networks now as i mentioned',\n","   'start': 1150.0,\n","   'duration': 3.12},\n","  {'text': 'neural networks will be pretty massive',\n","   'start': 1151.6,\n","   'duration': 3.6},\n","  {'text': 'expressions mathematical expressions so',\n","   'start': 1153.12,\n","   'duration': 3.36},\n","  {'text': 'we need some data structures that',\n","   'start': 1155.2,\n","   'duration': 2.719},\n","  {'text': \"maintain these expressions and that's\",\n","   'start': 1156.48,\n","   'duration': 2.88},\n","  {'text': \"what we're going to start to build out\",\n","   'start': 1157.919,\n","   'duration': 2.561},\n","  {'text': 'now', 'start': 1159.36, 'duration': 2.88},\n","  {'text': \"so we're going to\", 'start': 1160.48, 'duration': 3.6},\n","  {'text': 'build out this value object that i',\n","   'start': 1162.24,\n","   'duration': 4.0},\n","  {'text': 'showed you in the readme page of',\n","   'start': 1164.08,\n","   'duration': 3.44},\n","  {'text': 'micrograd', 'start': 1166.24, 'duration': 4.48},\n","  {'text': 'so let me copy paste a skeleton of the',\n","   'start': 1167.52,\n","   'duration': 6.0},\n","  {'text': 'first very simple value object',\n","   'start': 1170.72,\n","   'duration': 5.68},\n","  {'text': 'so class value takes a single', 'start': 1173.52, 'duration': 4.8},\n","  {'text': 'scalar value that it wraps and keeps',\n","   'start': 1176.4,\n","   'duration': 2.88},\n","  {'text': 'track of', 'start': 1178.32, 'duration': 3.12},\n","  {'text': \"and that's it so\", 'start': 1179.28, 'duration': 4.32},\n","  {'text': 'we can for example do value of 2.0 and',\n","   'start': 1181.44,\n","   'duration': 4.0},\n","  {'text': 'then we can', 'start': 1183.6, 'duration': 4.959},\n","  {'text': 'get we can look at its content and',\n","   'start': 1185.44,\n","   'duration': 4.96},\n","  {'text': 'python will internally', 'start': 1188.559, 'duration': 3.921},\n","  {'text': 'use the wrapper function', 'start': 1190.4, 'duration': 3.92},\n","  {'text': 'to uh return', 'start': 1192.48, 'duration': 4.4},\n","  {'text': 'uh this string oops', 'start': 1194.32, 'duration': 4.32},\n","  {'text': 'like that', 'start': 1196.88, 'duration': 3.919},\n","  {'text': 'so this is a value object with data',\n","   'start': 1198.64,\n","   'duration': 4.56},\n","  {'text': \"equals two that we're creating here\",\n","   'start': 1200.799,\n","   'duration': 4.161},\n","  {'text': \"now we'd like to do is like we'd like to\",\n","   'start': 1203.2,\n","   'duration': 4.24},\n","  {'text': 'be able to', 'start': 1204.96, 'duration': 5.12},\n","  {'text': 'have not just like two values',\n","   'start': 1207.44,\n","   'duration': 4.56},\n","  {'text': \"but we'd like to do a bluffy right we'd\",\n","   'start': 1210.08,\n","   'duration': 3.599},\n","  {'text': 'like to add them', 'start': 1212.0, 'duration': 3.44},\n","  {'text': 'so currently you would get an error',\n","   'start': 1213.679,\n","   'duration': 4.081},\n","  {'text': \"because python doesn't know how to add\",\n","   'start': 1215.44,\n","   'duration': 6.0},\n","  {'text': 'two value objects so we have to tell it',\n","   'start': 1217.76,\n","   'duration': 4.72},\n","  {'text': \"so here's\", 'start': 1221.44, 'duration': 3.44},\n","  {'text': 'addition', 'start': 1222.48, 'duration': 2.4},\n","  {'text': 'so you have to basically use these',\n","   'start': 1226.32,\n","   'duration': 3.2},\n","  {'text': 'special double underscore methods in',\n","   'start': 1227.6,\n","   'duration': 4.24},\n","  {'text': 'python to define these operators for',\n","   'start': 1229.52,\n","   'duration': 5.92},\n","  {'text': 'these objects so if we call um',\n","   'start': 1231.84,\n","   'duration': 7.28},\n","  {'text': 'the uh if we use this plus operator',\n","   'start': 1235.44,\n","   'duration': 7.599},\n","  {'text': 'python will internally call a dot add of',\n","   'start': 1239.12,\n","   'duration': 4.64},\n","  {'text': 'b', 'start': 1243.039, 'duration': 2.88},\n","  {'text': \"that's what will happen internally and\",\n","   'start': 1243.76,\n","   'duration': 5.2},\n","  {'text': 'so b will be the other and', 'start': 1245.919, 'duration': 5.041},\n","  {'text': 'self will be a', 'start': 1248.96, 'duration': 3.199},\n","  {'text': \"and so we see that what we're going to\",\n","   'start': 1250.96,\n","   'duration': 3.52},\n","  {'text': \"return is a new value object and it's\",\n","   'start': 1252.159,\n","   'duration': 4.561},\n","  {'text': \"just it's going to be wrapping\",\n","   'start': 1254.48,\n","   'duration': 4.079},\n","  {'text': 'the plus of', 'start': 1256.72, 'duration': 3.04},\n","  {'text': 'their data', 'start': 1258.559, 'duration': 3.441},\n","  {'text': 'but remember now because data is the',\n","   'start': 1259.76,\n","   'duration': 4.56},\n","  {'text': 'actual like numbered python number so',\n","   'start': 1262.0,\n","   'duration': 4.88},\n","  {'text': 'this operator here is just the typical',\n","   'start': 1264.32,\n","   'duration': 4.8},\n","  {'text': \"floating point plus addition now it's\",\n","   'start': 1266.88,\n","   'duration': 4.72},\n","  {'text': 'not an addition of value objects',\n","   'start': 1269.12,\n","   'duration': 5.12},\n","  {'text': 'and will return a new value so now a',\n","   'start': 1271.6,\n","   'duration': 4.4},\n","  {'text': 'plus b should work and it should print',\n","   'start': 1274.24,\n","   'duration': 2.96},\n","  {'text': 'value of', 'start': 1276.0, 'duration': 2.4},\n","  {'text': 'negative one', 'start': 1277.2, 'duration': 3.04},\n","  {'text': \"because that's two plus minus three\",\n","   'start': 1278.4,\n","   'duration': 3.279},\n","  {'text': 'there we go', 'start': 1280.24, 'duration': 4.0},\n","  {'text': \"okay let's now implement multiply\",\n","   'start': 1281.679,\n","   'duration': 4.24},\n","  {'text': 'just so we can recreate this expression',\n","   'start': 1284.24,\n","   'duration': 2.559},\n","  {'text': 'here', 'start': 1285.919, 'duration': 2.64},\n","  {'text': \"so multiply i think it won't surprise\",\n","   'start': 1286.799,\n","   'duration': 4.801},\n","  {'text': 'you will be fairly similar', 'start': 1288.559, 'duration': 4.641},\n","  {'text': \"so instead of add we're going to be\",\n","   'start': 1291.6,\n","   'duration': 2.72},\n","  {'text': 'using mul', 'start': 1293.2, 'duration': 2.8},\n","  {'text': 'and then here of course we want to do',\n","   'start': 1294.32,\n","   'duration': 2.56},\n","  {'text': 'times', 'start': 1296.0, 'duration': 2.64},\n","  {'text': 'and so now we can create a c value',\n","   'start': 1296.88,\n","   'duration': 4.159},\n","  {'text': 'object which will be 10.0 and now we',\n","   'start': 1298.64,\n","   'duration': 5.519},\n","  {'text': 'should be able to do a times b well',\n","   'start': 1301.039,\n","   'duration': 5.681},\n","  {'text': \"let's just do a times b first\",\n","   'start': 1304.159,\n","   'duration': 2.901},\n","  {'text': 'um', 'start': 1306.72, 'duration': 1.76},\n","  {'text': '[Music]', 'start': 1307.06, 'duration': 3.66},\n","  {'text': \"that's value of negative six now\",\n","   'start': 1308.48,\n","   'duration': 3.52},\n","  {'text': 'and by the way i skipped over this a',\n","   'start': 1310.72,\n","   'duration': 2.8},\n","  {'text': \"little bit suppose that i didn't have\",\n","   'start': 1312.0,\n","   'duration': 3.52},\n","  {'text': 'the wrapper function here', 'start': 1313.52, 'duration': 3.92},\n","  {'text': \"then it's just that you'll get some kind\",\n","   'start': 1315.52,\n","   'duration': 4.159},\n","  {'text': 'of an ugly expression so what wrapper is',\n","   'start': 1317.44,\n","   'duration': 4.64},\n","  {'text': \"doing is it's providing us a way to\",\n","   'start': 1319.679,\n","   'duration': 3.761},\n","  {'text': 'print out like a nicer looking',\n","   'start': 1322.08,\n","   'duration': 3.04},\n","  {'text': 'expression in python', 'start': 1323.44, 'duration': 3.76},\n","  {'text': \"uh so we don't just have something\",\n","   'start': 1325.12,\n","   'duration': 4.16},\n","  {'text': \"cryptic we actually are you know it's\",\n","   'start': 1327.2,\n","   'duration': 3.28},\n","  {'text': 'value of', 'start': 1329.28, 'duration': 4.72},\n","  {'text': 'negative six so this gives us a times',\n","   'start': 1330.48,\n","   'duration': 5.84},\n","  {'text': 'and then this we should now be able to',\n","   'start': 1334.0,\n","   'duration': 4.08},\n","  {'text': \"add c to it because we've defined and\",\n","   'start': 1336.32,\n","   'duration': 4.239},\n","  {'text': 'told the python how to do mul and add',\n","   'start': 1338.08,\n","   'duration': 4.32},\n","  {'text': 'and so this will call this will',\n","   'start': 1340.559,\n","   'duration': 4.321},\n","  {'text': 'basically be equivalent to a dot',\n","   'start': 1342.4,\n","   'duration': 4.48},\n","  {'text': 'small', 'start': 1344.88, 'duration': 3.039},\n","  {'text': 'of b', 'start': 1346.88, 'duration': 2.799},\n","  {'text': 'and then this new value object will be',\n","   'start': 1347.919,\n","   'duration': 3.361},\n","  {'text': 'dot add', 'start': 1349.679, 'duration': 2.88},\n","  {'text': 'of c', 'start': 1351.28, 'duration': 3.519},\n","  {'text': \"and so let's see if that worked\",\n","   'start': 1352.559,\n","   'duration': 3.841},\n","  {'text': 'yep so that worked well that gave us',\n","   'start': 1354.799,\n","   'duration': 4.401},\n","  {'text': 'four which is what we expect from before',\n","   'start': 1356.4,\n","   'duration': 4.56},\n","  {'text': 'and i believe we can just call them',\n","   'start': 1359.2,\n","   'duration': 5.12},\n","  {'text': 'manually as well there we go so',\n","   'start': 1360.96,\n","   'duration': 4.16},\n","  {'text': 'yeah', 'start': 1364.32, 'duration': 2.56},\n","  {'text': 'okay so now what we are missing is the',\n","   'start': 1365.12,\n","   'duration': 3.919},\n","  {'text': 'connective tissue of this expression as',\n","   'start': 1366.88,\n","   'duration': 3.36},\n","  {'text': 'i mentioned we want to keep these',\n","   'start': 1369.039,\n","   'duration': 3.52},\n","  {'text': 'expression graphs so we need to know and',\n","   'start': 1370.24,\n","   'duration': 4.72},\n","  {'text': 'keep pointers about what values produce',\n","   'start': 1372.559,\n","   'duration': 4.321},\n","  {'text': 'what other values', 'start': 1374.96, 'duration': 3.68},\n","  {'text': 'so here for example we are going to',\n","   'start': 1376.88,\n","   'duration': 3.2},\n","  {'text': \"introduce a new variable which we'll\",\n","   'start': 1378.64,\n","   'duration': 3.44},\n","  {'text': 'call children and by default it will be',\n","   'start': 1380.08,\n","   'duration': 3.44},\n","  {'text': 'an empty tuple', 'start': 1382.08, 'duration': 2.719},\n","  {'text': \"and then we're actually going to keep a\",\n","   'start': 1383.52,\n","   'duration': 2.88},\n","  {'text': 'slightly different variable in the class',\n","   'start': 1384.799,\n","   'duration': 3.76},\n","  {'text': \"which we'll call underscore prev which\",\n","   'start': 1386.4,\n","   'duration': 5.12},\n","  {'text': 'will be the set of children',\n","   'start': 1388.559,\n","   'duration': 4.721},\n","  {'text': 'this is how i done i did it in the',\n","   'start': 1391.52,\n","   'duration': 3.6},\n","  {'text': 'original micrograd looking at my code',\n","   'start': 1393.28,\n","   'duration': 3.84},\n","  {'text': \"here i can't remember exactly the reason\",\n","   'start': 1395.12,\n","   'duration': 4.0},\n","  {'text': 'i believe it was efficiency but this',\n","   'start': 1397.12,\n","   'duration': 3.84},\n","  {'text': 'underscore children will be a tuple for',\n","   'start': 1399.12,\n","   'duration': 3.28},\n","  {'text': 'convenience but then when we actually',\n","   'start': 1400.96,\n","   'duration': 2.959},\n","  {'text': 'maintain it in the class it will be just',\n","   'start': 1402.4,\n","   'duration': 5.2},\n","  {'text': 'this set yeah i believe for efficiency',\n","   'start': 1403.919,\n","   'duration': 4.961},\n","  {'text': 'um', 'start': 1407.6, 'duration': 2.319},\n","  {'text': 'so now', 'start': 1408.88, 'duration': 2.88},\n","  {'text': 'when we are creating a value like this',\n","   'start': 1409.919,\n","   'duration': 3.521},\n","  {'text': 'with a constructor children will be',\n","   'start': 1411.76,\n","   'duration': 4.32},\n","  {'text': 'empty and prep will be the empty set but',\n","   'start': 1413.44,\n","   'duration': 3.92},\n","  {'text': \"when we're creating a value through\",\n","   'start': 1416.08,\n","   'duration': 3.28},\n","  {'text': \"addition or multiplication we're going\",\n","   'start': 1417.36,\n","   'duration': 5.28},\n","  {'text': 'to feed in the children of this value',\n","   'start': 1419.36,\n","   'duration': 7.04},\n","  {'text': 'which in this case is self and other',\n","   'start': 1422.64,\n","   'duration': 5.44},\n","  {'text': 'so those are the children', 'start': 1426.4, 'duration': 4.24},\n","  {'text': 'here', 'start': 1428.08, 'duration': 4.88},\n","  {'text': 'so now we can do d dot prev', 'start': 1430.64, 'duration': 5.279},\n","  {'text': \"and we'll see that the children of the\",\n","   'start': 1432.96,\n","   'duration': 5.92},\n","  {'text': 'we now know are this value of negative 6',\n","   'start': 1435.919,\n","   'duration': 5.041},\n","  {'text': 'and value of 10 and this of course is',\n","   'start': 1438.88,\n","   'duration': 4.88},\n","  {'text': 'the value resulting from a times b and',\n","   'start': 1440.96,\n","   'duration': 5.599},\n","  {'text': 'the c value which is 10.', 'start': 1443.76, 'duration': 4.799},\n","  {'text': 'now the last piece of information we',\n","   'start': 1446.559,\n","   'duration': 4.081},\n","  {'text': \"don't know so we know that the children\",\n","   'start': 1448.559,\n","   'duration': 3.6},\n","  {'text': \"of every single value but we don't know\",\n","   'start': 1450.64,\n","   'duration': 4.08},\n","  {'text': 'what operation created this value',\n","   'start': 1452.159,\n","   'duration': 4.4},\n","  {'text': \"so we need one more element here let's\",\n","   'start': 1454.72,\n","   'duration': 4.4},\n","  {'text': 'call it underscore pop', 'start': 1456.559, 'duration': 4.641},\n","  {'text': 'and by default this is the empty set for',\n","   'start': 1459.12,\n","   'duration': 3.2},\n","  {'text': 'leaves', 'start': 1461.2, 'duration': 4.4},\n","  {'text': \"and then we'll just maintain it here\",\n","   'start': 1462.32,\n","   'duration': 5.04},\n","  {'text': 'and now the operation will be just a',\n","   'start': 1465.6,\n","   'duration': 3.92},\n","  {'text': 'simple string and in the case of',\n","   'start': 1467.36,\n","   'duration': 3.919},\n","  {'text': \"addition it's plus in the case of\",\n","   'start': 1469.52,\n","   'duration': 4.32},\n","  {'text': 'multiplication is times', 'start': 1471.279, 'duration': 3.921},\n","  {'text': 'so now we', 'start': 1473.84, 'duration': 3.199},\n","  {'text': 'not just have d dot pref we also have a',\n","   'start': 1475.2,\n","   'duration': 3.44},\n","  {'text': 'd dot up', 'start': 1477.039, 'duration': 3.201},\n","  {'text': 'and we know that d was produced by an',\n","   'start': 1478.64,\n","   'duration': 4.159},\n","  {'text': 'addition of those two values and so now',\n","   'start': 1480.24,\n","   'duration': 3.76},\n","  {'text': 'we have the full', 'start': 1482.799, 'duration': 3.281},\n","  {'text': \"mathematical expression uh and we're\",\n","   'start': 1484.0,\n","   'duration': 3.52},\n","  {'text': 'building out this data structure and we',\n","   'start': 1486.08,\n","   'duration': 3.839},\n","  {'text': 'know exactly how each value came to be',\n","   'start': 1487.52,\n","   'duration': 4.08},\n","  {'text': 'by word expression and from what other',\n","   'start': 1489.919,\n","   'duration': 4.081},\n","  {'text': 'values', 'start': 1491.6, 'duration': 2.4},\n","  {'text': 'now because these expressions are about',\n","   'start': 1494.72,\n","   'duration': 3.36},\n","  {'text': \"to get quite a bit larger we'd like a\",\n","   'start': 1496.159,\n","   'duration': 4.241},\n","  {'text': 'way to nicely visualize these',\n","   'start': 1498.08,\n","   'duration': 4.16},\n","  {'text': \"expressions that we're building out so\",\n","   'start': 1500.4,\n","   'duration': 3.44},\n","  {'text': \"for that i'm going to copy paste a bunch\",\n","   'start': 1502.24,\n","   'duration': 4.24},\n","  {'text': \"of slightly scary code that's going to\",\n","   'start': 1503.84,\n","   'duration': 4.8},\n","  {'text': 'visualize this these expression graphs',\n","   'start': 1506.48,\n","   'duration': 3.12},\n","  {'text': 'for us', 'start': 1508.64, 'duration': 2.399},\n","  {'text': \"so here's the code and i'll explain it\",\n","   'start': 1509.6,\n","   'duration': 3.76},\n","  {'text': 'in a bit but first let me just show you',\n","   'start': 1511.039,\n","   'duration': 3.841},\n","  {'text': 'what this code does', 'start': 1513.36, 'duration': 3.36},\n","  {'text': 'basically what it does is it creates a',\n","   'start': 1514.88,\n","   'duration': 4.24},\n","  {'text': 'new function drawdot that we can call on',\n","   'start': 1516.72,\n","   'duration': 4.0},\n","  {'text': 'some root node', 'start': 1519.12, 'duration': 3.6},\n","  {'text': \"and then it's going to visualize it so\",\n","   'start': 1520.72,\n","   'duration': 4.0},\n","  {'text': 'if we call drawdot on d', 'start': 1522.72, 'duration': 4.319},\n","  {'text': 'which is this final value here that is a',\n","   'start': 1524.72,\n","   'duration': 5.04},\n","  {'text': 'times b plus c', 'start': 1527.039, 'duration': 4.401},\n","  {'text': 'it creates something like this so this',\n","   'start': 1529.76,\n","   'duration': 2.48},\n","  {'text': 'is d', 'start': 1531.44, 'duration': 3.2},\n","  {'text': 'and you see that this is a times b',\n","   'start': 1532.24,\n","   'duration': 4.72},\n","  {'text': 'creating an integrated value plus c',\n","   'start': 1534.64,\n","   'duration': 5.76},\n","  {'text': 'gives us this output node d', 'start': 1536.96, 'duration': 5.44},\n","  {'text': \"so that's dried out of d\", 'start': 1540.4, 'duration': 3.92},\n","  {'text': \"and i'm not going to go through this in\",\n","   'start': 1542.4,\n","   'duration': 3.759},\n","  {'text': 'complete detail you can take a look at',\n","   'start': 1544.32,\n","   'duration': 4.64},\n","  {'text': 'graphless and its api uh graphis is a',\n","   'start': 1546.159,\n","   'duration': 5.201},\n","  {'text': 'open source graph visualization software',\n","   'start': 1548.96,\n","   'duration': 3.52},\n","  {'text': \"and what we're doing here is we're\",\n","   'start': 1551.36,\n","   'duration': 3.48},\n","  {'text': 'building out this graph and graphis',\n","   'start': 1552.48,\n","   'duration': 4.48},\n","  {'text': 'api and', 'start': 1554.84, 'duration': 4.04},\n","  {'text': 'you can basically see that trace is this',\n","   'start': 1556.96,\n","   'duration': 3.839},\n","  {'text': 'helper function that enumerates all of',\n","   'start': 1558.88,\n","   'duration': 3.919},\n","  {'text': 'the nodes and edges in the graph',\n","   'start': 1560.799,\n","   'duration': 3.441},\n","  {'text': 'so that just builds a set of all the',\n","   'start': 1562.799,\n","   'duration': 3.281},\n","  {'text': 'nodes and edges and then we iterate for',\n","   'start': 1564.24,\n","   'duration': 3.76},\n","  {'text': 'all the nodes and we create special node',\n","   'start': 1566.08,\n","   'duration': 2.8},\n","  {'text': 'objects', 'start': 1568.0, 'duration': 3.36},\n","  {'text': 'for them in', 'start': 1568.88, 'duration': 4.24},\n","  {'text': 'using dot node', 'start': 1571.36, 'duration': 4.319},\n","  {'text': 'and then we also create edges using dot',\n","   'start': 1573.12,\n","   'duration': 3.679},\n","  {'text': 'dot edge', 'start': 1575.679, 'duration': 2.401},\n","  {'text': \"and the only thing that's like slightly\",\n","   'start': 1576.799,\n","   'duration': 3.601},\n","  {'text': \"tricky here is you'll notice that i\",\n","   'start': 1578.08,\n","   'duration': 4.32},\n","  {'text': 'basically add these fake nodes which are',\n","   'start': 1580.4,\n","   'duration': 4.32},\n","  {'text': 'these operation nodes so for example',\n","   'start': 1582.4,\n","   'duration': 5.279},\n","  {'text': 'this node here is just like a plus node',\n","   'start': 1584.72,\n","   'duration': 4.079},\n","  {'text': 'and', 'start': 1587.679, 'duration': 4.081},\n","  {'text': 'i create these', 'start': 1588.799, 'duration': 5.601},\n","  {'text': 'special op nodes here', 'start': 1591.76, 'duration': 5.919},\n","  {'text': 'and i connect them accordingly so these',\n","   'start': 1594.4,\n","   'duration': 5.12},\n","  {'text': 'nodes of course are not actual',\n","   'start': 1597.679,\n","   'duration': 4.0},\n","  {'text': 'nodes in the original graph', 'start': 1599.52, 'duration': 4.399},\n","  {'text': \"they're not actually a value object the\",\n","   'start': 1601.679,\n","   'duration': 4.48},\n","  {'text': 'only value objects here are the things',\n","   'start': 1603.919,\n","   'duration': 4.24},\n","  {'text': 'in squares those are actual value',\n","   'start': 1606.159,\n","   'duration': 4.4},\n","  {'text': 'objects or representations thereof and',\n","   'start': 1608.159,\n","   'duration': 4.241},\n","  {'text': 'these op nodes are just created in this',\n","   'start': 1610.559,\n","   'duration': 4.961},\n","  {'text': 'drawdot routine so that it looks nice',\n","   'start': 1612.4,\n","   'duration': 5.04},\n","  {'text': \"let's also add labels to these graphs\",\n","   'start': 1615.52,\n","   'duration': 4.399},\n","  {'text': 'just so we know what variables are where',\n","   'start': 1617.44,\n","   'duration': 4.32},\n","  {'text': \"so let's create a special underscore\",\n","   'start': 1619.919,\n","   'duration': 2.961},\n","  {'text': 'label', 'start': 1621.76, 'duration': 2.159},\n","  {'text': 'um', 'start': 1622.88, 'duration': 2.56},\n","  {'text': \"or let's just do label\", 'start': 1623.919, 'duration': 4.481},\n","  {'text': 'equals empty by default and save it in',\n","   'start': 1625.44,\n","   'duration': 5.44},\n","  {'text': 'each node', 'start': 1628.4, 'duration': 2.48},\n","  {'text': \"and then here we're going to do label as\",\n","   'start': 1631.279,\n","   'duration': 4.081},\n","  {'text': 'a', 'start': 1633.279, 'duration': 4.561},\n","  {'text': 'label is the', 'start': 1635.36, 'duration': 6.039},\n","  {'text': 'label a c', 'start': 1637.84, 'duration': 3.559},\n","  {'text': 'and then', 'start': 1642.799, 'duration': 4.88},\n","  {'text': \"let's create a special um\", 'start': 1644.799, 'duration': 5.921},\n","  {'text': 'e equals a times b', 'start': 1647.679, 'duration': 3.041},\n","  {'text': 'and e dot label will be e', 'start': 1650.799, 'duration': 4.88},\n","  {'text': \"it's kind of naughty\", 'start': 1654.08, 'duration': 4.079},\n","  {'text': 'and e will be e plus c', 'start': 1655.679, 'duration': 5.12},\n","  {'text': 'and a d dot label will be', 'start': 1658.159, 'duration': 4.4},\n","  {'text': 'd', 'start': 1660.799, 'duration': 3.441},\n","  {'text': 'okay so nothing really changes i just',\n","   'start': 1662.559,\n","   'duration': 3.84},\n","  {'text': 'added this new e function', 'start': 1664.24, 'duration': 4.24},\n","  {'text': 'a new e variable', 'start': 1666.399, 'duration': 4.16},\n","  {'text': 'and then here when we are', 'start': 1668.48, 'duration': 3.36},\n","  {'text': 'printing this', 'start': 1670.559, 'duration': 3.681},\n","  {'text': \"i'm going to print the label here so\",\n","   'start': 1671.84,\n","   'duration': 4.24},\n","  {'text': 'this will be a percent s', 'start': 1674.24, 'duration': 2.64},\n","  {'text': 'bar', 'start': 1676.08, 'duration': 4.24},\n","  {'text': 'and this will be end.label', 'start': 1676.88, 'duration': 3.44},\n","  {'text': 'and so now', 'start': 1681.279, 'duration': 3.921},\n","  {'text': 'we have the label on the left here so it',\n","   'start': 1683.44,\n","   'duration': 4.239},\n","  {'text': 'says a b creating e and then e plus c',\n","   'start': 1685.2,\n","   'duration': 3.52},\n","  {'text': 'creates d', 'start': 1687.679, 'duration': 3.12},\n","  {'text': 'just like we have it here', 'start': 1688.72, 'duration': 3.6},\n","  {'text': \"and finally let's make this expression\",\n","   'start': 1690.799,\n","   'duration': 3.521},\n","  {'text': 'just one layer deeper', 'start': 1692.32, 'duration': 4.88},\n","  {'text': 'so d will not be the final output node',\n","   'start': 1694.32,\n","   'duration': 5.68},\n","  {'text': 'instead after d we are going to create a',\n","   'start': 1697.2,\n","   'duration': 4.56},\n","  {'text': 'new value object', 'start': 1700.0, 'duration': 3.679},\n","  {'text': \"called f we're going to start running\",\n","   'start': 1701.76,\n","   'duration': 4.159},\n","  {'text': 'out of variables soon f will be negative',\n","   'start': 1703.679,\n","   'duration': 3.681},\n","  {'text': '2.0', 'start': 1705.919, 'duration': 4.64},\n","  {'text': 'and its label will of course just be f',\n","   'start': 1707.36,\n","   'duration': 6.799},\n","  {'text': 'and then l capital l will be the output',\n","   'start': 1710.559,\n","   'duration': 4.881},\n","  {'text': 'of our graph', 'start': 1714.159, 'duration': 3.841},\n","  {'text': 'and l will be p times f', 'start': 1715.44, 'duration': 3.359},\n","  {'text': 'okay', 'start': 1718.0, 'duration': 2.32},\n","  {'text': 'so l will be negative eight is the',\n","   'start': 1718.799,\n","   'duration': 3.921},\n","  {'text': 'output', 'start': 1720.32, 'duration': 2.4},\n","  {'text': 'so', 'start': 1722.84, 'duration': 6.68},\n","  {'text': \"now we don't just draw a d we draw l\",\n","   'start': 1724.72,\n","   'duration': 4.8},\n","  {'text': 'okay', 'start': 1730.0, 'duration': 4.0},\n","  {'text': 'and somehow the label of', 'start': 1732.0, 'duration': 4.399},\n","  {'text': 'l was undefined oops all that label has',\n","   'start': 1734.0,\n","   'duration': 5.679},\n","  {'text': 'to be explicitly sort of given to it',\n","   'start': 1736.399,\n","   'duration': 5.28},\n","  {'text': 'there we go so l is the output',\n","   'start': 1739.679,\n","   'duration': 3.521},\n","  {'text': \"so let's quickly recap what we've done\",\n","   'start': 1741.679,\n","   'duration': 2.401},\n","  {'text': 'so far', 'start': 1743.2, 'duration': 2.479},\n","  {'text': 'we are able to build out mathematical',\n","   'start': 1744.08,\n","   'duration': 3.92},\n","  {'text': 'expressions using only plus and times so',\n","   'start': 1745.679,\n","   'duration': 3.521},\n","  {'text': 'far', 'start': 1748.0, 'duration': 3.52},\n","  {'text': 'they are scalar valued along the way',\n","   'start': 1749.2,\n","   'duration': 5.04},\n","  {'text': 'and we can do this forward pass',\n","   'start': 1751.52,\n","   'duration': 4.879},\n","  {'text': 'and build out a mathematical expression',\n","   'start': 1754.24,\n","   'duration': 4.4},\n","  {'text': 'so we have multiple inputs here a b c',\n","   'start': 1756.399,\n","   'duration': 3.121},\n","  {'text': 'and f', 'start': 1758.64, 'duration': 2.88},\n","  {'text': 'going into a mathematical expression',\n","   'start': 1759.52,\n","   'duration': 4.48},\n","  {'text': 'that produces a single output l',\n","   'start': 1761.52,\n","   'duration': 4.879},\n","  {'text': 'and this here is visualizing the forward',\n","   'start': 1764.0,\n","   'duration': 4.72},\n","  {'text': 'pass so the output of the forward pass',\n","   'start': 1766.399,\n","   'duration': 5.201},\n","  {'text': \"is negative eight that's the value\",\n","   'start': 1768.72,\n","   'duration': 4.559},\n","  {'text': \"now what we'd like to do next is we'd\",\n","   'start': 1771.6,\n","   'duration': 3.76},\n","  {'text': 'like to run back propagation',\n","   'start': 1773.279,\n","   'duration': 3.921},\n","  {'text': 'and in back propagation we are going to',\n","   'start': 1775.36,\n","   'duration': 4.24},\n","  {'text': \"start here at the end and we're going to\",\n","   'start': 1777.2,\n","   'duration': 3.68},\n","  {'text': 'reverse', 'start': 1779.6, 'duration': 3.92},\n","  {'text': 'and calculate the gradient along along',\n","   'start': 1780.88,\n","   'duration': 4.56},\n","  {'text': 'all these intermediate values',\n","   'start': 1783.52,\n","   'duration': 3.279},\n","  {'text': \"and really what we're computing for\",\n","   'start': 1785.44,\n","   'duration': 3.44},\n","  {'text': 'every single value here', 'start': 1786.799, 'duration': 4.161},\n","  {'text': \"um we're going to compute the derivative\",\n","   'start': 1788.88,\n","   'duration': 6.48},\n","  {'text': 'of that node with respect to l',\n","   'start': 1790.96,\n","   'duration': 5.199},\n","  {'text': 'so', 'start': 1795.36, 'duration': 3.36},\n","  {'text': 'the derivative of l with respect to l is',\n","   'start': 1796.159,\n","   'duration': 4.321},\n","  {'text': 'just uh one', 'start': 1798.72, 'duration': 3.04},\n","  {'text': \"and then we're going to derive what is\",\n","   'start': 1800.48,\n","   'duration': 3.36},\n","  {'text': 'the derivative of l with respect to f',\n","   'start': 1801.76,\n","   'duration': 4.56},\n","  {'text': 'with respect to d with respect to c with',\n","   'start': 1803.84,\n","   'duration': 3.76},\n","  {'text': 'respect to e', 'start': 1806.32, 'duration': 4.0},\n","  {'text': 'with respect to b and with respect to a',\n","   'start': 1807.6,\n","   'duration': 4.48},\n","  {'text': \"and in the neural network setting you'd\",\n","   'start': 1810.32,\n","   'duration': 3.52},\n","  {'text': 'be very interested in the derivative of',\n","   'start': 1812.08,\n","   'duration': 4.64},\n","  {'text': 'basically this loss function l',\n","   'start': 1813.84,\n","   'duration': 4.559},\n","  {'text': 'with respect to the weights of a neural',\n","   'start': 1816.72,\n","   'duration': 2.64},\n","  {'text': 'network', 'start': 1818.399, 'duration': 2.321},\n","  {'text': 'and here of course we have just these',\n","   'start': 1819.36,\n","   'duration': 3.039},\n","  {'text': 'variables a b c and f', 'start': 1820.72, 'duration': 3.12},\n","  {'text': 'but some of these will eventually',\n","   'start': 1822.399,\n","   'duration': 3.361},\n","  {'text': 'represent the weights of a neural net',\n","   'start': 1823.84,\n","   'duration': 3.439},\n","  {'text': \"and so we'll need to know how those\",\n","   'start': 1825.76,\n","   'duration': 3.36},\n","  {'text': 'weights are impacting', 'start': 1827.279, 'duration': 3.76},\n","  {'text': \"the loss function so we'll be interested\",\n","   'start': 1829.12,\n","   'duration': 3.12},\n","  {'text': 'basically in the derivative of the',\n","   'start': 1831.039,\n","   'duration': 3.601},\n","  {'text': 'output with respect to some of its leaf',\n","   'start': 1832.24,\n","   'duration': 4.319},\n","  {'text': 'nodes and those leaf nodes will be the',\n","   'start': 1834.64,\n","   'duration': 3.44},\n","  {'text': 'weights of the neural net', 'start': 1836.559, 'duration': 2.961},\n","  {'text': 'and the other leaf nodes of course will',\n","   'start': 1838.08,\n","   'duration': 3.599},\n","  {'text': 'be the data itself but usually we will',\n","   'start': 1839.52,\n","   'duration': 4.48},\n","  {'text': 'not want or use the derivative of the',\n","   'start': 1841.679,\n","   'duration': 4.081},\n","  {'text': 'loss function with respect to data',\n","   'start': 1844.0,\n","   'duration': 3.76},\n","  {'text': 'because the data is fixed but the',\n","   'start': 1845.76,\n","   'duration': 4.88},\n","  {'text': 'weights will be iterated on', 'start': 1847.76, 'duration': 4.799},\n","  {'text': 'using the gradient information so next',\n","   'start': 1850.64,\n","   'duration': 3.6},\n","  {'text': 'we are going to create a variable inside',\n","   'start': 1852.559,\n","   'duration': 4.48},\n","  {'text': 'the value class that maintains the',\n","   'start': 1854.24,\n","   'duration': 5.679},\n","  {'text': 'derivative of l with respect to that',\n","   'start': 1857.039,\n","   'duration': 3.921},\n","  {'text': 'value', 'start': 1859.919, 'duration': 3.841},\n","  {'text': 'and we will call this variable grad',\n","   'start': 1860.96,\n","   'duration': 4.88},\n","  {'text': \"so there's a data and there's a\",\n","   'start': 1863.76,\n","   'duration': 3.44},\n","  {'text': 'self.grad', 'start': 1865.84, 'duration': 3.76},\n","  {'text': 'and initially it will be zero and',\n","   'start': 1867.2,\n","   'duration': 5.04},\n","  {'text': 'remember that zero is basically means no',\n","   'start': 1869.6,\n","   'duration': 4.72},\n","  {'text': \"effect so at initialization we're\",\n","   'start': 1872.24,\n","   'duration': 3.84},\n","  {'text': 'assuming that every value does not',\n","   'start': 1874.32,\n","   'duration': 4.239},\n","  {'text': 'impact does not affect the out the',\n","   'start': 1876.08,\n","   'duration': 3.52},\n","  {'text': 'output', 'start': 1878.559, 'duration': 2.801},\n","  {'text': 'right because if the gradient is zero',\n","   'start': 1879.6,\n","   'duration': 3.6},\n","  {'text': 'that means that changing this variable',\n","   'start': 1881.36,\n","   'duration': 4.319},\n","  {'text': 'is not changing the loss function',\n","   'start': 1883.2,\n","   'duration': 4.079},\n","  {'text': 'so by default we assume that the',\n","   'start': 1885.679,\n","   'duration': 3.201},\n","  {'text': 'gradient is zero', 'start': 1887.279, 'duration': 3.921},\n","  {'text': 'and then', 'start': 1888.88, 'duration': 7.279},\n","  {'text': \"now that we have grad and it's 0.0\",\n","   'start': 1891.2,\n","   'duration': 4.959},\n","  {'text': 'we are going to be able to visualize it',\n","   'start': 1896.559,\n","   'duration': 6.24},\n","  {'text': 'here after data so here grad is 0.4 f',\n","   'start': 1898.24,\n","   'duration': 7.439},\n","  {'text': 'and this will be in that graph',\n","   'start': 1902.799,\n","   'duration': 4.801},\n","  {'text': 'and now we are going to be showing both',\n","   'start': 1905.679,\n","   'duration': 5.281},\n","  {'text': 'the data and the grad', 'start': 1907.6, 'duration': 6.079},\n","  {'text': 'initialized at zero', 'start': 1910.96, 'duration': 4.719},\n","  {'text': 'and we are just about getting ready to',\n","   'start': 1913.679,\n","   'duration': 3.681},\n","  {'text': 'calculate the back propagation',\n","   'start': 1915.679,\n","   'duration': 3.12},\n","  {'text': 'and of course this grad again as i',\n","   'start': 1917.36,\n","   'duration': 3.039},\n","  {'text': 'mentioned is representing', 'start': 1918.799, 'duration': 3.441},\n","  {'text': 'the derivative of the output in this',\n","   'start': 1920.399,\n","   'duration': 4.64},\n","  {'text': 'case l with respect to this value so',\n","   'start': 1922.24,\n","   'duration': 4.24},\n","  {'text': 'with respect to so this is the',\n","   'start': 1925.039,\n","   'duration': 3.52},\n","  {'text': 'derivative of l with respect to f with',\n","   'start': 1926.48,\n","   'duration': 4.88},\n","  {'text': \"respect to d and so on so let's now fill\",\n","   'start': 1928.559,\n","   'duration': 4.401},\n","  {'text': 'in those gradients and actually do back',\n","   'start': 1931.36,\n","   'duration': 3.36},\n","  {'text': \"propagation manually so let's start\",\n","   'start': 1932.96,\n","   'duration': 3.439},\n","  {'text': 'filling in these gradients and start all',\n","   'start': 1934.72,\n","   'duration': 3.76},\n","  {'text': 'the way at the end as i mentioned here',\n","   'start': 1936.399,\n","   'duration': 4.0},\n","  {'text': 'first we are interested to fill in this',\n","   'start': 1938.48,\n","   'duration': 4.4},\n","  {'text': 'gradient here so what is the derivative',\n","   'start': 1940.399,\n","   'duration': 4.801},\n","  {'text': 'of l with respect to l', 'start': 1942.88, 'duration': 4.48},\n","  {'text': 'in other words if i change l by a tiny',\n","   'start': 1945.2,\n","   'duration': 3.839},\n","  {'text': 'amount of h', 'start': 1947.36, 'duration': 3.199},\n","  {'text': 'how much does', 'start': 1949.039, 'duration': 3.281},\n","  {'text': 'l change', 'start': 1950.559, 'duration': 4.641},\n","  {'text': \"it changes by h so it's proportional and\",\n","   'start': 1952.32,\n","   'duration': 4.959},\n","  {'text': 'therefore derivative will be one',\n","   'start': 1955.2,\n","   'duration': 4.0},\n","  {'text': 'we can of course measure these or',\n","   'start': 1957.279,\n","   'duration': 3.681},\n","  {'text': 'estimate these numerical gradients',\n","   'start': 1959.2,\n","   'duration': 4.0},\n","  {'text': \"numerically just like we've seen before\",\n","   'start': 1960.96,\n","   'duration': 4.319},\n","  {'text': 'so if i take this expression', 'start': 1963.2, 'duration': 6.079},\n","  {'text': 'and i create a def lol function here',\n","   'start': 1965.279,\n","   'duration': 5.921},\n","  {'text': \"and put this here now the reason i'm\",\n","   'start': 1969.279,\n","   'duration': 4.321},\n","  {'text': 'creating a gating function hello here is',\n","   'start': 1971.2,\n","   'duration': 4.0},\n","  {'text': \"because i don't want to pollute or mess\",\n","   'start': 1973.6,\n","   'duration': 3.6},\n","  {'text': 'up the global scope here this is just',\n","   'start': 1975.2,\n","   'duration': 3.68},\n","  {'text': 'kind of like a little staging area and',\n","   'start': 1977.2,\n","   'duration': 3.199},\n","  {'text': 'as you know in python all of these will',\n","   'start': 1978.88,\n","   'duration': 3.84},\n","  {'text': 'be local variables to this function so',\n","   'start': 1980.399,\n","   'duration': 4.321},\n","  {'text': \"i'm not changing any of the global scope\",\n","   'start': 1982.72,\n","   'duration': 3.04},\n","  {'text': 'here', 'start': 1984.72, 'duration': 5.28},\n","  {'text': 'so here l1 will be l', 'start': 1985.76, 'duration': 7.279},\n","  {'text': 'and then copy pasting this expression',\n","   'start': 1990.0,\n","   'duration': 6.88},\n","  {'text': \"we're going to add a small amount h\",\n","   'start': 1993.039,\n","   'duration': 3.841},\n","  {'text': 'in for example a', 'start': 1997.36, 'duration': 5.28},\n","  {'text': 'right and this would be measuring the',\n","   'start': 2000.559,\n","   'duration': 4.881},\n","  {'text': 'derivative of l with respect to a',\n","   'start': 2002.64,\n","   'duration': 5.519},\n","  {'text': 'so here this will be l2', 'start': 2005.44, 'duration': 4.079},\n","  {'text': 'and then we want to print this',\n","   'start': 2008.159,\n","   'duration': 3.441},\n","  {'text': 'derivative so print', 'start': 2009.519, 'duration': 5.681},\n","  {'text': 'l2 minus l1 which is how much l changed',\n","   'start': 2011.6,\n","   'duration': 6.079},\n","  {'text': 'and then normalize it by h so this is',\n","   'start': 2015.2,\n","   'duration': 4.319},\n","  {'text': 'the rise over run', 'start': 2017.679, 'duration': 3.681},\n","  {'text': 'and we have to be careful because l is a',\n","   'start': 2019.519,\n","   'duration': 5.601},\n","  {'text': 'value node so we actually want its data',\n","   'start': 2021.36,\n","   'duration': 4.88},\n","  {'text': 'um', 'start': 2025.12, 'duration': 3.76},\n","  {'text': 'so that these are floats dividing by h',\n","   'start': 2026.24,\n","   'duration': 4.48},\n","  {'text': 'and this should print the derivative of',\n","   'start': 2028.88,\n","   'duration': 4.159},\n","  {'text': 'l with respect to a because a is the one',\n","   'start': 2030.72,\n","   'duration': 4.88},\n","  {'text': 'that we bumped a little bit by h',\n","   'start': 2033.039,\n","   'duration': 4.24},\n","  {'text': 'so what is the', 'start': 2035.6, 'duration': 3.679},\n","  {'text': 'derivative of l with respect to a',\n","   'start': 2037.279,\n","   'duration': 3.76},\n","  {'text': \"it's six\", 'start': 2039.279, 'duration': 4.321},\n","  ...],\n"," 'comments': [{'author': '@jiahaosu',\n","   'text': 'I&#39;m mind blow by the sheer clarity &amp; simiplicity of the explaination.',\n","   'likes': 0,\n","   'published_at': '2024-12-05T13:22:17Z'},\n","  {'author': '@NexusCoderX',\n","   'text': '*<a href=\"https://www.youtube.com/watch?v=VMj-3S1tku0&amp;t=1311\">21:51</a> use of repr function',\n","   'likes': 0,\n","   'published_at': '2024-12-03T16:29:14Z'},\n","  {'author': '@davidpulcifer3792',\n","   'text': 'Amazing video!',\n","   'likes': 0,\n","   'published_at': '2024-12-02T19:15:10Z'},\n","  {'author': '@roeybh',\n","   'text': 'Thanks!',\n","   'likes': 0,\n","   'published_at': '2024-12-01T10:54:00Z'},\n","  {'author': '@eraybaydemir9689',\n","   'text': 'something I dont get it in <a href=\"https://www.youtube.com/watch?v=VMj-3S1tku0&amp;t=7000\">1:56:40</a> is that when we draw our nodes, there are bunch of nodes that are not having any parent node so im guessing that they are input data nodes but why arent they are grouped up to better represent visuality or is it just how liblary works or they are not input nodes?? I would be more than grateful if someone can able to explain this.',\n","   'likes': 0,\n","   'published_at': '2024-11-30T23:43:24Z'},\n","  {'author': '@haukasonic',\n","   'text': 'Thanks!',\n","   'likes': 0,\n","   'published_at': '2024-11-30T16:06:30Z'},\n","  {'author': '@marufaytekin6302',\n","   'text': 'Thank you, Andrej, for your incredible work in simplifying the complexities of neural networks for others to understand easily. Your ability to make such advanced topics so clear and accessible is truly inspiring—this video is invaluable!  <br>&quot;Simplicity is the ultimate sophistication.&quot; – Leonardo da Vinci',\n","   'likes': 0,\n","   'published_at': '2024-11-29T21:59:28Z'},\n","  {'author': '@pedromillet6672',\n","   'text': 'Whoowww... Being here without coding exp and still can go through it. What a masterclass.',\n","   'likes': 0,\n","   'published_at': '2024-11-27T22:46:53Z'},\n","  {'author': '@worksmarter6418',\n","   'text': 'Why are we += the grad? The discussed error was not zeroing the grad. But, since we are only assigning the grad with _backward once per node, why do we not have it say = ... Instead of +=?',\n","   'likes': 0,\n","   'published_at': '2024-11-26T21:31:31Z'},\n","  {'author': '@shreyaagarwal1123',\n","   'text': 'really thank u for making me understand  how neural network actually works makes me think how useless my uni actually is',\n","   'likes': 0,\n","   'published_at': '2024-11-24T10:35:48Z'},\n","  {'author': '@nelliparthiajith3118',\n","   'text': 'When I run the following command from micrograd_lecture_second_half_roughly.ipynb:\\r<br>\\r<br>[(yout - ygt)**2 for ygt, yout in zip(ys, ypred)]\\r<br>\\r<br>I get a clean output:\\r<br>\\r<br>[Value(data=1.8688676392069992)), Value(data=0.37661915959598025)), Value(data=0.13611849326555403)), Value(data=1.69235533142263))]\\r<br>\\r<br>But whenever I tried to enclose it in a sum() function:\\r<br>\\r<br>sum((yout - ygt)**2 for ygt, yout in zip(ys, ypred))\\r<br>\\r<br>It throws an error:\\r<br>\\r<br>TypeError: unsupported operand type(s) for +: &#39;int&#39; and &#39;Value&#39;',\n","   'likes': 0,\n","   'published_at': '2024-11-22T13:15:55Z'},\n","  {'author': '@AmeerHussain-f8j',\n","   'text': 'This is probably the most useful YouTube video I&#39;ve ever seen.',\n","   'likes': 0,\n","   'published_at': '2024-11-21T21:26:01Z'},\n","  {'author': '@helloansuman',\n","   'text': 'Keep uploading...',\n","   'likes': 0,\n","   'published_at': '2024-11-21T18:57:18Z'},\n","  {'author': '@leobianco9116',\n","   'text': 'This is truly valuable, thank you.',\n","   'likes': 0,\n","   'published_at': '2024-11-21T18:47:17Z'},\n","  {'author': '@School_of_Technology',\n","   'text': '<a href=\"https://www.youtube.com/watch?v=VMj-3S1tku0&amp;t=7912\">2:11:52</a> - I was able to understand the bug you explained - after going through the video second time',\n","   'likes': 0,\n","   'published_at': '2024-11-21T18:35:15Z'},\n","  {'author': '@massimomattia16',\n","   'text': 'great',\n","   'likes': 0,\n","   'published_at': '2024-11-21T18:19:57Z'},\n","  {'author': '@shreyasmanolkar',\n","   'text': 'At <a href=\"https://www.youtube.com/watch?v=VMj-3S1tku0&amp;t=6890\">1:54:50</a> if some one is facing with error of: <br><br> &quot;TypeError: unsupported operand type(s) for +: &#39;int&#39; and &#39;Value&#39;&quot;&quot;<br><br><b>*try with this solution:*</b><br>loss = sum(((yout - ygt)**2 for ygt, yout in zip(ys, ypred)), Value(0.0))<br><br><b>*Explanation:*</b><br>The error is happening because Python&#39;s built-in sum() function starts with an initial value of 0 (an integer) and tries to add Value objects to it. We need to provide an initial value that&#39;s a Value object.',\n","   'likes': 0,\n","   'published_at': '2024-11-21T07:16:41Z'},\n","  {'author': '@querela92',\n","   'text': 'Thank you. Just the introduction about derivatives in those NN libraries opened my eyes. I was always wondering how it&#39;s implemented without using symbols and formulas.',\n","   'likes': 0,\n","   'published_at': '2024-11-20T20:16:31Z'},\n","  {'author': '@DJBaruah',\n","   'text': 'Absolutely gorgeous ❤',\n","   'likes': 0,\n","   'published_at': '2024-11-20T17:24:06Z'},\n","  {'author': '@lingfengguan9252',\n","   'text': 'Hi Andrej, first of all, thanks for putting up this awesome video. It&#39;s amazing. <br>I have a little question around <a href=\"https://www.youtube.com/watch?v=VMj-3S1tku0&amp;t=7939\">2:12:19</a>, when you fix the bug using .zero_grad(). <br>When I was trying to implement my own backward() function earlier, I did the equivalent of .zero_grad() before I backward propagate the derivatives. However, since I saw both you and pytorch decided that it should be a separate function, I&#39;m wondering what&#39;s the reason for not doing it in the backward() function itself?<br><br> I&#39;m also thinking only resetting the grad of the parameters may cause other issues in more complicated cases. e.g. If a neuron used a different activation function, and some constant Value was involved in the formula which never gets zeroed during the iterations, and thus causing a hard to notice bug.',\n","   'likes': 0,\n","   'published_at': '2024-11-18T01:53:01Z'},\n","  {'author': '@thantzinmaung-yq6cu',\n","   'text': 'what do I do if I could not build this? I feel very bad because everyone here saying they understood it well but I did not.',\n","   'likes': 0,\n","   'published_at': '2024-11-18T01:40:49Z'},\n","  {'author': '@FooBar-nv3kt',\n","   'text': 'you hit your keyboard too loud. feels like i&#39;m getting punched on each keystroke.',\n","   'likes': 0,\n","   'published_at': '2024-11-17T07:07:52Z'},\n","  {'author': '@pavalep',\n","   'text': 'Thanks Andrej👍',\n","   'likes': 0,\n","   'published_at': '2024-11-16T17:38:41Z'},\n","  {'author': '@squawterry',\n","   'text': 'I would take every class you taught at Stanford.',\n","   'likes': 0,\n","   'published_at': '2024-11-15T16:58:40Z'},\n","  {'author': '@amanslair',\n","   'text': 'The buggy optimization without zero grad actually implemented the momentum algorithm and may actually be why it reached a much lower loss!',\n","   'likes': 0,\n","   'published_at': '2024-11-11T04:12:52Z'},\n","  {'author': '@jasonzhang6534',\n","   'text': 'andrej, you are the ideal professor that we want. this video is simple, intuitive and just amazing!',\n","   'likes': 0,\n","   'published_at': '2024-11-10T16:16:44Z'},\n","  {'author': '@carlosgruss7289',\n","   'text': 'Crazy that some of the most talented/knowledgeable people on the planet just come out here on YouTube and offer to teach the world for free. Makes you feel hopeful about humanity in a way :)',\n","   'likes': 10,\n","   'published_at': '2024-11-09T09:06:48Z'},\n","  {'author': '@anantanurag3087',\n","   'text': 'Thanks!',\n","   'likes': 1,\n","   'published_at': '2024-11-09T06:38:29Z'},\n","  {'author': '@watt_the_border_collie',\n","   'text': 'i managed to alter the code, so I don&#39;t evaluate each expression, I just build the graph that can be evaluated by calling the eval() function<br>Is it possible to do the same for the gradients?<br><br>meaning if I have:<br>a = Value(4)<br>b = Value(2)<br><br>c = a*b<br>d = c + b<br>out = d**2<br><br>and I want to be able to say: out.grad &lt;-- should be something like: [d out / da, d.out / db]',\n","   'likes': 0,\n","   'published_at': '2024-11-08T18:00:52Z'},\n","  {'author': '@bingbingsun6304',\n","   'text': 'I tried to write my own backpropagation before, I have the same bug for a variable used multiple times, ^_^',\n","   'likes': 0,\n","   'published_at': '2024-11-05T14:54:20Z'},\n","  {'author': '@LuvRatan-x2s',\n","   'text': 'Hi... please like my comment to remind me to watch this video.',\n","   'likes': 0,\n","   'published_at': '2024-11-02T18:30:47Z'},\n","  {'author': '@InfernoVortex334',\n","   'text': 'Thank you sir.',\n","   'likes': 0,\n","   'published_at': '2024-11-01T19:57:41Z'},\n","  {'author': '@KeithAdams-p8z',\n","   'text': 'thank you for making these videos',\n","   'likes': 0,\n","   'published_at': '2024-11-01T06:21:54Z'},\n","  {'author': '@rando6645',\n","   'text': 'Is the loss used appropriately here? sum((y_pred-y_true) ** 2 for ....) actually computing the sum and not the mean? Shouldn&#39;t it be something like mean((y_pred-y_true) **2)?',\n","   'likes': 0,\n","   'published_at': '2024-10-30T19:44:19Z'},\n","  {'author': '@bluearctik3980',\n","   'text': 'The level of the pedagogy here is incredible -- thank you so much for putting this out to the world!',\n","   'likes': 0,\n","   'published_at': '2024-10-27T21:52:09Z'},\n","  {'author': '@satyajit1512',\n","   'text': 'Very humble way of teaching!',\n","   'likes': 0,\n","   'published_at': '2024-10-27T16:07:34Z'},\n","  {'author': '@LouisNwadike-u2h',\n","   'text': 'Thanks for this awesome content Andrej. You&#39;ve over simplified NNs and MLPs. You&#39;re a genius, man 👌',\n","   'likes': 0,\n","   'published_at': '2024-10-27T03:32:25Z'},\n","  {'author': '@gautamgoel2024',\n","   'text': 'If anybody facing error in <i>_call_</i> function in Neuron class, then may use this for sum() function:  act = sum([wi * xi for wi, xi in zip(self.w, x)], Value(0.0)) + self.b',\n","   'likes': 0,\n","   'published_at': '2024-10-26T16:57:11Z'},\n","  {'author': '@skillme7777',\n","   'text': 'the explanation are vague, the instructor doesn&#39;t explain why he does certain things. I didn&#39;t find the resource to be too good. explain all videos in more detail with simple words.',\n","   'likes': 1,\n","   'published_at': '2024-10-25T16:27:03Z'},\n","  {'author': '@reallylordofnothing',\n","   'text': 'This is the most fascinating math class on derivatives I never had.',\n","   'likes': 0,\n","   'published_at': '2024-10-25T13:37:38Z'},\n","  {'author': '@PrukkzBR',\n","   'text': 'i don&#39;t have maturity to hear u saying dn, my bad',\n","   'likes': 0,\n","   'published_at': '2024-10-25T00:35:05Z'},\n","  {'author': '@justchary',\n","   'text': 'This learning video is invaluable for humanity! What really excites\\xa0me here is not only to see the final result, but to see how a top industry guy thinks and uses his tools (e.g. python, Jupyter\\xa0notebook)',\n","   'likes': 0,\n","   'published_at': '2024-10-22T11:54:50Z'},\n","  {'author': '@dynox666',\n","   'text': 'absolutely the best! Thanks',\n","   'likes': 0,\n","   'published_at': '2024-10-20T02:52:14Z'},\n","  {'author': '@NBbhanu',\n","   'text': 'this is a great tutorial. For those who are new to the mathematics, best take a calculus class first and come back to this to understand the lecture better. I did so and it makes a lot of sense after.',\n","   'likes': 1,\n","   'published_at': '2024-10-15T03:06:49Z'},\n","  {'author': '@jiangnan8998',\n","   'text': 'Good teaching',\n","   'likes': 0,\n","   'published_at': '2024-10-14T14:17:44Z'},\n","  {'author': '@TheMrPippo',\n","   'text': 'Awesome video! Thank you, Andrej!',\n","   'likes': 0,\n","   'published_at': '2024-10-13T21:02:05Z'},\n","  {'author': '@tee_iam78',\n","   'text': 'a very beautiful lecture. thank you.',\n","   'likes': 0,\n","   'published_at': '2024-10-13T13:52:33Z'},\n","  {'author': '@abhishekswain2502',\n","   'text': 'I felt more value added to this when I first watched your cs231n lecture 4 (<a href=\"https://www.youtube.com/watch?v=i94OvYb6noo\">https://www.youtube.com/watch?v=i94OvYb6noo&amp;t=3906s)</a> and then came to this. Thanks a lot Andrej!',\n","   'likes': 0,\n","   'published_at': '2024-10-11T22:49:09Z'},\n","  {'author': '@foo_tube',\n","   'text': 'I didn&#39;t quite understand why we can&#39;t implement __rpow__....',\n","   'likes': 0,\n","   'published_at': '2024-10-11T22:46:10Z'},\n","  {'author': '@nicolasvair',\n","   'text': 'That was a great one, thank you ! 🙏',\n","   'likes': 0,\n","   'published_at': '2024-10-11T19:13:51Z'},\n","  {'author': '@AN-oq5he',\n","   'text': 'am i the only 14 year old watching this',\n","   'likes': 0,\n","   'published_at': '2024-10-10T20:39:15Z'},\n","  {'author': '@sumanthsumanth7114',\n","   'text': '❤❤❤',\n","   'likes': 1,\n","   'published_at': '2024-10-10T17:39:48Z'},\n","  {'author': '@e_hossam96',\n","   'text': 'This is very great! Thank you 😇',\n","   'likes': 0,\n","   'published_at': '2024-10-10T00:00:06Z'},\n","  {'author': '@himanshupatidar007',\n","   'text': 'People who really want to teach others, will always give it for freee!',\n","   'likes': 0,\n","   'published_at': '2024-10-08T04:16:50Z'},\n","  {'author': '@TheArrowster',\n","   'text': 'What a memorable video! It was exactly the explanation I was looking for! I literally have tears in my eyes because I finally learned neural networks the right way! Thank you so much for sharing, Andrej! Greetings from Brazil!!!',\n","   'likes': 0,\n","   'published_at': '2024-10-07T20:07:37Z'},\n","  {'author': '@rish5591',\n","   'text': 'Thank You for this video :)',\n","   'likes': 0,\n","   'published_at': '2024-10-06T18:41:23Z'},\n","  {'author': '@Snehilw',\n","   'text': 'Nothing but gratitude for you Andrej. I love to watch all your talks and explanations. Very refreshing, energizing, motivating, clear and concise. I can watch 100 hrs of your talks at a stretch without loosing attention. Very captivating! Kudos! And thank you for this great community service!',\n","   'likes': 0,\n","   'published_at': '2024-10-06T04:22:34Z'},\n","  {'author': '@rikukobayashi3220',\n","   'text': 'Thank you so much Andrej!',\n","   'likes': 0,\n","   'published_at': '2024-10-05T19:44:01Z'},\n","  {'author': '@nayanvats3424',\n","   'text': 'All work, no Gibberish. The Karpathy way <a href=\"UCkszU2WH9gy1mb0dV-11UJg/popcorn-yellow-striped-smile\"></a>',\n","   'likes': 0,\n","   'published_at': '2024-10-05T05:48:13Z'},\n","  {'author': '@AN-oq5he',\n","   'text': 'this is peak',\n","   'likes': 0,\n","   'published_at': '2024-10-05T04:10:31Z'},\n","  {'author': '@vishalbhat8630',\n","   'text': 'I have a doubt using a set for storing children would not store duplicate values so if i do the same operation twice with this it wouldnt be stored right',\n","   'likes': 1,\n","   'published_at': '2024-10-04T16:52:32Z'},\n","  {'author': '@yuzheng7799',\n","   'text': 'Thanks!',\n","   'likes': 0,\n","   'published_at': '2024-10-04T05:49:44Z'},\n","  {'author': '@BodhiiDharma',\n","   'text': 'Thank you',\n","   'likes': 0,\n","   'published_at': '2024-10-03T18:46:33Z'},\n","  {'author': '@ichsanulamal19',\n","   'text': 'bro just doing magic, i love you andrej',\n","   'likes': 0,\n","   'published_at': '2024-10-03T02:33:41Z'},\n","  {'author': '@KarthikD-dx9um',\n","   'text': 'I have no hesitation in saying that this is the world &#39;s best video for neural nets.Salute to your efforts🤗',\n","   'likes': 0,\n","   'published_at': '2024-10-02T14:44:17Z'},\n","  {'author': '@peterhirt991',\n","   'text': 'Excellent! thanks a lot',\n","   'likes': 0,\n","   'published_at': '2024-10-01T17:01:09Z'},\n","  {'author': '@shashidharkudari5613',\n","   'text': 'We are blessed to get a great explanation from the such great AI figure for free, Thank you very very much',\n","   'likes': 0,\n","   'published_at': '2024-10-01T10:30:35Z'},\n","  {'author': '@rahuldasari1886',\n","   'text': 'Excellent explanation',\n","   'likes': 0,\n","   'published_at': '2024-09-28T14:29:25Z'},\n","  {'author': '@beko9099',\n","   'text': 'Bob Ross of Machine Learning',\n","   'likes': 1,\n","   'published_at': '2024-09-28T14:28:19Z'},\n","  {'author': '@skadauke',\n","   'text': 'I loved the bloopers at the end! Amazing lecture, one of the best videos I&#39;ve ever seen on YouTube.',\n","   'likes': 0,\n","   'published_at': '2024-09-26T18:04:13Z'},\n","  {'author': '@tianwang',\n","   'text': 'Unbelievably useful and comprehensive, you are a great teacher!',\n","   'likes': 0,\n","   'published_at': '2024-09-25T21:04:57Z'},\n","  {'author': '@tommymarshall69',\n","   'text': '@AndrejKarpathy At <a href=\"https://www.youtube.com/watch?v=VMj-3S1tku0&amp;t=4662\">01:17:42</a>  you use topological sort to make sure the `._backward()` calls are executed in the correct order. Before you revealed the answer I had a go at coming up with a solution myself, and came up with a simple recursive function:<br><br>```<br>def backprop(v):<br>    root.grad = 1.0<br>    def backp(v):<br>        v._backward()<br>        if v._prev:<br>            for child in v._prev:<br>                backp(child)<br>    backp(v)<br>backprop(o)<br>```<br><br>Is this correct? If yes, is there any particular reason why topological sort is better?',\n","   'likes': 0,\n","   'published_at': '2024-09-25T19:45:11Z'},\n","  {'author': '@tianwang',\n","   'text': '<a href=\"https://www.youtube.com/watch?v=VMj-3S1tku0&amp;t=6180\">1:43:00</a> how did we set the grad of variable o to be 1.0 (or any other value) before we call backward()? or it&#39;s set to 1.0 by default (which is odd?)<br><br>EDIT: oh I found out, you just need to pass (gradient=xx) to backward() function or it&#39;s default to 1.0.',\n","   'likes': 0,\n","   'published_at': '2024-09-25T18:37:13Z'},\n","  {'author': '@johnmcdonald4514',\n","   'text': 'Many thanks for taking the time to share Andrej - I recently completed a MIT course on Designing and Building AI products/solutions, and your simple, concrete examples were invaluable to tie many of the concepts together.  Well done.',\n","   'likes': 0,\n","   'published_at': '2024-09-25T11:31:19Z'},\n","  {'author': '@OnkarNora',\n","   'text': 'first 15 mins and the guy litrally told me why derivatives were thought in my school',\n","   'likes': 0,\n","   'published_at': '2024-09-25T05:59:10Z'},\n","  {'author': '@prakash-eu7ui',\n","   'text': 'I can&#39;t believe myself, I finally understood neural network',\n","   'likes': 0,\n","   'published_at': '2024-09-23T18:03:41Z'},\n","  {'author': '@ADMINADMIN-j7s',\n","   'text': 'I watched this video series so many times, I finally understand it',\n","   'likes': 0,\n","   'published_at': '2024-09-23T14:09:58Z'},\n","  {'author': '@vishalakshi6140',\n","   'text': 'thank you so so much Andrej! I love you and thank you for being who you are, thank you for being the teacher you are, and you inspire me to become a better teacher in life! ❤❤❤❤❤❤❤❤❤❤❤❤❤',\n","   'likes': 0,\n","   'published_at': '2024-09-22T21:33:21Z'},\n","  {'author': '@aieverythingsfine',\n","   'text': 'Thanks bro',\n","   'likes': 0,\n","   'published_at': '2024-09-21T05:54:44Z'},\n","  {'author': '@lu0142',\n","   'text': '这是我第一次看完长达两个半小时的视频，虽然是分了三次。受益匪浅，同时也明白 andrej 为什么创业去做教育相关的事情。\\r<br>视频对 nn 知识面的分解，对每个知识点的讲解，都非常不错！同时也非常期待 andrej 后续的创业产品。',\n","   'likes': 0,\n","   'published_at': '2024-09-20T00:29:27Z'},\n","  {'author': '@OvidioPereraMoser',\n","   'text': 'Can micrograd be used to develop a neural network to classify the digits from the  EMNIST dataset? I tried but did not know how to write the loss function for it. I did write a graph using VectorFlow. Thanks.',\n","   'likes': 0,\n","   'published_at': '2024-09-18T21:48:32Z'},\n","  {'author': '@DailyProg',\n","   'text': 'Sentence 1: so let’s start from the very beginning <br>Sentence 2: let’s first implement an electron before implementing a CPU so we can implement a neuron',\n","   'likes': 0,\n","   'published_at': '2024-09-18T17:04:31Z'},\n","  {'author': '@minxie1078',\n","   'text': 'Absolutely the best NN hands-on class. Thanks Andrej!',\n","   'likes': 0,\n","   'published_at': '2024-09-18T00:03:57Z'},\n","  {'author': '@epsilon2944',\n","   'text': 'What was his random function doing? :D  At <a href=\"https://www.youtube.com/watch?v=VMj-3S1tku0&amp;t=6465\">1:47:45</a> Generating 0.9xx like 5 times and at <a href=\"https://www.youtube.com/watch?v=VMj-3S1tku0&amp;t=6700\">1:51:40</a> generating 0.88 3 times. And to the topic - thank you Andrej for this. When I see good people like you transferring the knowledge to the new generation of AI developers, I&#39;m quite a bit more optimistic about our future. I think with more people like you we can create responsible AI.',\n","   'likes': 0,\n","   'published_at': '2024-09-17T05:21:31Z'},\n","  {'author': '@siddhantverma532',\n","   'text': 'Instead of doing the topological sort and then calling _backward() on each node. Why can&#39;t we call ._backward for each child inside _.backward() method. When the backward is invoked for one node, it will keep calling .backward for each children till it reaches all leaf nodes.',\n","   'likes': 0,\n","   'published_at': '2024-09-15T15:11:06Z'},\n","  {'author': '@ssooyyeerr',\n","   'text': 'For me this was far better explanation than <a href=\"https://www.youtube.com/watch?v=SmZmBKc7Lrs\">https://www.youtube.com/watch?v=SmZmBKc7Lrs&amp;lc=UgyKKzfDxJCl_-sCI-V4AaABAg</a> or <a href=\"https://www.youtube.com/watch?v=Ilg3gGewQ5U&amp;list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&amp;index=3\">https://www.youtube.com/watch?v=Ilg3gGewQ5U&amp;list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&amp;index=3</a> or in the Grokking Deep Learning book.<br>Thanks!',\n","   'likes': 1,\n","   'published_at': '2024-09-13T14:37:45Z'},\n","  {'author': '@BradIsenbek',\n","   'text': 'One of the most important series of lessons and code that exists rn.',\n","   'likes': 0,\n","   'published_at': '2024-09-12T00:06:46Z'},\n","  {'author': '@SharangKulkarni',\n","   'text': '<a href=\"https://www.youtube.com/watch?v=ANVjvsRMWoon\">https://www.youtube.com/watch?v=ANVjvsRMWoon</a> this is indeed helpful',\n","   'likes': 1,\n","   'published_at': '2024-09-11T13:43:33Z'},\n","  {'author': '@JaazFelicio',\n","   'text': 'I&#39;ve been re-watching this class for a while, and so far, I&#39;m still haven&#39;t mastered it',\n","   'likes': 1,\n","   'published_at': '2024-09-10T16:59:55Z'},\n","  {'author': '@mrlemhen',\n","   'text': 'Awesome!',\n","   'likes': 0,\n","   'published_at': '2024-09-08T18:17:24Z'},\n","  {'author': '@Томас-э6о',\n","   'text': 'Thanks to Andrej Karpathy for this awesome video!!!',\n","   'likes': 0,\n","   'published_at': '2024-09-08T17:58:31Z'},\n","  {'author': '@davidespinosa1910',\n","   'text': 'Here&#39;s an example that shows why we need topsort:<br>b = a + 1<br>out = b + b<br>Using naive DFS:<br>out.grad = 1<br>b.grad += out.grad  (now b.grad = 1)<br>a.grad += b.grad  (now a.grad = 1)<br>b.grad += out.grad  (now b.grad = 2)<br>a.grad += b.grad   (now a.grad = 3, which is wrong)',\n","   'likes': 0,\n","   'published_at': '2024-09-08T13:52:25Z'},\n","  {'author': '@davidespinosa1910',\n","   'text': 'Surprisingly, this video illustrates how mathematicians work:  <b>they do examples until the answer is obvious</b> .',\n","   'likes': 1,\n","   'published_at': '2024-09-08T10:32:03Z'},\n","  {'author': '@andrii.kukuruza',\n","   'text': 'Truly appreciate your work, Andrej! Thank you VERY MUCH for the video!',\n","   'likes': 0,\n","   'published_at': '2024-09-08T09:33:54Z'},\n","  {'author': '@muhammadfaizan8839',\n","   'text': 'It is great tutorial ever I watched on Neural Network&#39;s implementation. Thank you Sir Andrej Karpathy for your effort and time. Please make more and more videos on Deep learning so that we can learn from you.',\n","   'likes': 0,\n","   'published_at': '2024-09-08T05:32:29Z'},\n","  {'author': '@K9Megahertz',\n","   'text': '<a href=\"https://www.youtube.com/watch?v=VMj-3S1tku0&amp;t=3157\">52:37</a> - I&#39;m getting a different value here. I come up with -8.266496. Anyone else getting the same?',\n","   'likes': 0,\n","   'published_at': '2024-09-07T19:39:13Z'},\n","  {'author': '@Mortz76',\n","   'text': 'simply the best!',\n","   'likes': 0,\n","   'published_at': '2024-09-06T15:13:31Z'},\n","  {'author': '@jindeng9890',\n","   'text': 'thank you so much for this insightful video!',\n","   'likes': 0,\n","   'published_at': '2024-09-06T08:37:35Z'},\n","  {'author': '@RachitMehta-pm2jm',\n","   'text': 'Thank You !! This is simply Amazingg &lt;3. Just love , respect and admiration for you Sir<br>!',\n","   'likes': 0,\n","   'published_at': '2024-09-05T21:51:02Z'},\n","  {'author': '@vallard-',\n","   'text': 'amazing.  Thank you. Why do you use the tanh function as the very last step?  Is that cause you want the output to be between -1 and 1? And if so, why? I didn&#39;t quite catch that. Thanks again!',\n","   'likes': 0,\n","   'published_at': '2024-09-05T17:09:22Z'}]}"]},"metadata":{},"execution_count":32}]},{"cell_type":"markdown","source":["# **Task 2 - Help you learn the key insights from video and comments,  and start your learning/insights/understanding**"],"metadata":{"id":"P-HRQUlaahQd"}},{"cell_type":"code","source":["class MajorInsights(pg.Object):  ## Your output, you define it\n","  summary: str\n","  key_insights: str\n","  questions: str\n","\n","major_insights = lf.query(prompt = \"{{video_data}}\", video_data=video_data, schema=MajorInsights, lm=lm_openai)"],"metadata":{"id":"Q7dhd6EWXnmE","executionInfo":{"status":"ok","timestamp":1733438092461,"user_tz":480,"elapsed":27776,"user":{"displayName":"amy shi","userId":"00695257177288089305"}}},"execution_count":53,"outputs":[]},{"cell_type":"code","source":["major_insights"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":442},"id":"JxoOLXgiYDf0","executionInfo":{"status":"ok","timestamp":1733438095064,"user_tz":480,"elapsed":135,"user":{"displayName":"amy shi","userId":"00695257177288089305"}},"outputId":"1611888b-5fa8-4e44-fc93-cfb2163f727a"},"execution_count":54,"outputs":[{"output_type":"execute_result","data":{"text/plain":["MajorInsights(summary=\"The video provides an in-depth walkthrough of building a neural network from scratch using Python, focusing on backpropagation and automatic differentiation. The tutorial, led by Andrej Karpathy, introduces the micrograd library, which is a minimalist autograd engine for understanding neural networks' internals. The content is structured to gradually build up the complexity, starting from basic calculus concepts to implementing a full neural network with gradient descent optimization.\", key_insights='1. The tutorial emphasizes the importance of understanding derivatives in neural network training and backpropagation. \\n2. Micrograd is used to illustrate how to implement autograd functionality with minimal code, making the concepts more accessible. \\n3. The video covers essential operations such as addition, multiplication, and non-linear activation functions like tanh, and explains their derivatives. \\n4. A significant focus is placed on understanding the chain rule and its application in backpropagation. \\n5. The video highlights common pitfalls in neural network training, such as forgetting to zero gradients before backpropagation. \\n6. Practical insights into neural network optimization are provided, including discussions on learning rate tuning and gradient descent.', questions='1. How does micrograd compare to more complex libraries like PyTorch in terms of functionality and performance? \\n2. What are the advantages and limitations of using a scalar-based autograd engine like micrograd for learning purposes? \\n3. How can the concepts taught in this tutorial be scaled to more complex neural network architectures and datasets? \\n4. What are some best practices for tuning hyperparameters like learning rate in neural network training? \\n5. How does the choice of activation functions affect the training and performance of neural networks?')"],"text/html":["<html>\n","<head>\n","<style>\n","/* Tooltip styles. */\n","span.tooltip {\n","  visibility: hidden;\n","  white-space: pre-wrap;\n","  font-weight: normal;\n","  background-color: #484848;\n","  color: #fff;\n","  padding: 10px;\n","  border-radius: 6px;\n","  position: absolute;\n","  z-index: 1;\n","}\n","span.tooltip:hover {\n","  visibility: visible;\n","}\n","/* Summary styles. */\n","details.pyglove summary {\n","  font-weight: bold;\n","  margin: -0.5em -0.5em 0;\n","  padding: 0.5em;\n","}\n",".summary-name {\n","  display: inline;\n","  padding: 3px 5px 3px 5px;\n","  margin: 0 5px;\n","  border-radius: 3px;\n","}\n",".summary-title {\n","  display: inline;\n","}\n",".summary-name + div.summary-title {\n","  display: inline;\n","  color: #aaa;\n","}\n",".summary-title:hover + span.tooltip {\n","  visibility: visible;\n","}\n",".summary-name:hover > span.tooltip {\n","  visibility: visible;\n","  background-color: darkblue;\n","}\n","/* Simple value styles. */\n",".simple-value {\n","  color: blue;\n","  display: inline-block;\n","  white-space: pre-wrap;\n","  padding: 0.2em;\n","  margin-top: 0.15em;\n","}\n",".simple-value.str {\n","  color: darkred;\n","  font-style: italic;\n","}\n",".simple-value.int, .simple-value.float {\n","  color: darkblue;\n","}\n","/* Value details styles. */\n","details.pyglove {\n","  border: 1px solid #aaa;\n","  border-radius: 4px;\n","  padding: 0.5em 0.5em 0;\n","  margin: 0.25em 0;\n","}\n","details.pyglove[open] {\n","  padding: 0.5em 0.5em 0.5em;\n","}\n",".highlight {\n","  background-color: Mark;\n","}\n",".lowlight {\n","  opacity: 0.2;\n","}\n","/* Complex value styles. */\n","span.empty-container::before {\n","    content: '(empty)';\n","    font-style: italic;\n","    margin-left: 0.5em;\n","    color: #aaa;\n","}\n","</style>\n","</head>\n","<body>\n","<details open class=\"pyglove major-insights\"><summary><div class=\"summary-title\">MajorInsights(...)</div><span class=\"tooltip\">MajorInsights(\n","  summary=&#x27;The video provides an in-depth walkthrough of building a neural network from scratch using Python, focusing on backpropagation and automatic differentiation. The tutorial, led by Andrej Karpathy, introduces the micrograd library, which is a minimalist auto...&#x27;,\n","  key_insights=&#x27;1. The tutorial emphasizes the importance of understanding derivatives in neural network training and backpropagation. \\n2. Micrograd is used to illustrate how to implement autograd functionality with minimal code, making the concepts more accessible. \\n3. T...&#x27;,\n","  questions=&#x27;1. How does micrograd compare to more complex libraries like PyTorch in terms of functionality and performance? \\n2. What are the advantages and limitations of using a scalar-based autograd engine like micrograd for learning purposes? \\n3. How can the concep...&#x27;\n",")</span></summary><div class=\"complex-value major-insights\"><details open class=\"pyglove str\"><summary><div class=\"summary-name\">summary<span class=\"tooltip\">summary</span></div><div class=\"summary-title\">str</div><span class=\"tooltip\">&#x27;The video provides an in-depth walkthrough of building a neural network from scratch using Python, focusing on backpropagation and automatic differentiation. The tutorial, led by Andrej Karpathy, introduces the micrograd library, which is a minimalist auto...&#x27;</span></summary><span class=\"simple-value str\">The video provides an in-depth walkthrough of building a neural network from scratch using Python, focusing on backpropagation and automatic differentiation. The tutorial, led by Andrej Karpathy, introduces the micrograd library, which is a minimalist autograd engine for understanding neural networks&#x27; internals. The content is structured to gradually build up the complexity, starting from basic calculus concepts to implementing a full neural network with gradient descent optimization.</span></details><details open class=\"pyglove str\"><summary><div class=\"summary-name\">key_insights<span class=\"tooltip\">key_insights</span></div><div class=\"summary-title\">str</div><span class=\"tooltip\">&#x27;1. The tutorial emphasizes the importance of understanding derivatives in neural network training and backpropagation. \\n2. Micrograd is used to illustrate how to implement autograd functionality with minimal code, making the concepts more accessible. \\n3. T...&#x27;</span></summary><span class=\"simple-value str\">1. The tutorial emphasizes the importance of understanding derivatives in neural network training and backpropagation. \n","2. Micrograd is used to illustrate how to implement autograd functionality with minimal code, making the concepts more accessible. \n","3. The video covers essential operations such as addition, multiplication, and non-linear activation functions like tanh, and explains their derivatives. \n","4. A significant focus is placed on understanding the chain rule and its application in backpropagation. \n","5. The video highlights common pitfalls in neural network training, such as forgetting to zero gradients before backpropagation. \n","6. Practical insights into neural network optimization are provided, including discussions on learning rate tuning and gradient descent.</span></details><details open class=\"pyglove str\"><summary><div class=\"summary-name\">questions<span class=\"tooltip\">questions</span></div><div class=\"summary-title\">str</div><span class=\"tooltip\">&#x27;1. How does micrograd compare to more complex libraries like PyTorch in terms of functionality and performance? \\n2. What are the advantages and limitations of using a scalar-based autograd engine like micrograd for learning purposes? \\n3. How can the concep...&#x27;</span></summary><span class=\"simple-value str\">1. How does micrograd compare to more complex libraries like PyTorch in terms of functionality and performance? \n","2. What are the advantages and limitations of using a scalar-based autograd engine like micrograd for learning purposes? \n","3. How can the concepts taught in this tutorial be scaled to more complex neural network architectures and datasets? \n","4. What are some best practices for tuning hyperparameters like learning rate in neural network training? \n","5. How does the choice of activation functions affect the training and performance of neural networks?</span></details></div></details>\n","</body>\n","</html>"]},"metadata":{},"execution_count":54}]}]}